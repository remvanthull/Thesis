{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Kopie van Jema.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"9NiWcXXgaII1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624477915742,"user_tz":-120,"elapsed":2199,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}},"outputId":"9617fe21-e1d7-44b7-a632-2b3e2d8c6c74"},"source":["import numpy as np\n","import pandas as pd\n","import random\n","import string\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from scipy.optimize import minimize\n","import math\n","import json\n","from nltk import tokenize\n","import collections\n","import re\n","import sys\n","import itertools\n","import time\n","import nltk\n","from scipy.stats import mannwhitneyu\n","\n","import statsmodels.api as sm\n","from statsmodels.base.model import GenericLikelihoodModel,\\\n","        GenericLikelihoodModelResults\n","\n","from statsmodels.nonparametric.smoothers_lowess import lowess\n","\n","from scipy.special import zeta\n","from scipy.stats import binom\n","\n","import pickle\n","import seaborn as sns\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","lg = np.log10\n","\n","from scipy.stats import chisquare"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YY8qnjNcaLlT","executionInfo":{"status":"ok","timestamp":1624477935759,"user_tz":-120,"elapsed":20025,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}},"outputId":"369e2463-e168-458f-b720-4a32eb113927"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nWYxII2u3Fja","executionInfo":{"status":"ok","timestamp":1624477938402,"user_tz":-120,"elapsed":2650,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["gpt_set = pickle.load(open(\"/content/drive/MyDrive/datasets/gpt_set.p\", \"rb\" ))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"tO8rqhvtaII5","executionInfo":{"status":"ok","timestamp":1624477940536,"user_tz":-120,"elapsed":2136,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["human_set = pickle.load(open(\"/content/drive/MyDrive/datasets/human_set.p\", \"rb\" ))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VyqED3IOaII8"},"source":["# Subsampling"]},{"cell_type":"code","metadata":{"id":"DbuyiwS0aII8","executionInfo":{"status":"ok","timestamp":1624477944112,"user_tz":-120,"elapsed":227,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Returns 2 lists of corpora, one from which the ranks will be calculated\n","# and one from which the frequencies will be calculated. Each corpus consists of\n","# a list of tokenized sentences.\n","# Input: corpus that is to be subsampled. Should be a list of tokenized sentences.\n","# k is the amount of tokens that each sampled corpus should contain,\n","# m is the amount of subcorpera you want for both the ranks and frequencies.\n","# Max: I would read Valentin's thesis for an explanation on subsampling\n","def subsampling(corpus, k = 1000000, m = 10, sent = True):\n","    n = len(corpus)\n","    \n","    sen_len = {}\n","\n","    \n","    rank_corpera = []\n","    freq_corpera = []\n","\n","    if sent:\n","        for i in range(m):\n","            used_rank = set()\n","            used_freq = set()\n","            rank_count = 0\n","            freq_count = 0\n","            rank_samples = []\n","            freq_samples = []\n","\n","            while rank_count < k:\n","                index = np.random.randint(n)\n","                if index in used_rank:\n","                    continue\n","\n","                rank_sample = corpus[index]\n","                len_sample = len(rank_sample)\n","\n","                if len_sample == 0:\n","                    continue\n","\n","                if rank_count > k:\n","                    max_len = len_sample - (rank_count - k)\n","                    rank_sample = rank_sample[:max_len]\n","                    \n","                rank_samples += rank_sample\n","                rank_count += len_sample\n","\n","\n","                used_rank.add(index)\n","\n","            while freq_count < k:\n","                index = np.random.randint(n)\n","                if index in used_freq:\n","                    continue\n","                freq_sample = corpus[index]\n","                len_sample = len(freq_sample)\n","\n","                if len_sample == 0:\n","                    continue\n","                    \n","                if freq_count > k:\n","                    max_len = len_sample - (freq_count - k)\n","                    freq_sample = freq_sample[:max_len]\n","\n","                freq_samples += freq_sample\n","                freq_count += len_sample\n","\n","                if len_sample not in sen_len and len_sample < 200:\n","                    sen_len[len_sample] = 1\n","                elif len_sample < 200:\n","                    sen_len[len_sample] += 1\n","\n","                used_freq.add(index)\n","\n","            rank_corpera.append(rank_samples)\n","            freq_corpera.append(freq_samples)\n","#                 rank_corpera.append([item for sublist in rank_samples for item in sublist])\n","#                 freq_corpera.append([item for sublist in freq_samples for item in sublist])\n","\n","\n","    else:\n","        for i in range(m):\n","            rank_samples = random.sample(corpus, k)\n","            freq_samples = random.sample(corpus, k)\n","            rank_corpera.append(rank_samples)\n","            freq_corpera.append(freq_samples)\n","    \n","#     return rank_corpera, freq_corpera, sen_len\n","    return rank_corpera, freq_corpera"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZorEQNoaII-","executionInfo":{"status":"ok","timestamp":1624477946312,"user_tz":-120,"elapsed":4,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Returns a dataframe of word frequencies for list of corpora,\n","# with each column corresponding to a different corpus.\n","# Input: list of corpora. Each corpus consists of a list of tokenized sentences.\n","def calculate_freqs(freq_sents, norm=True, text=None):\n","    freq_dict = {}\n","    norm_dict = {}\n","    for i, corpus in enumerate(freq_sents):\n","        freq_dict['{} c_frequency {}'.format(text,i)] = collections.Counter(corpus)\n","        if norm:\n","            len_corp = len(corpus)\n","            norm_dict['{} c_frequency {}'.format(text, i)] = {k: v / len_corp for k, v in freq_dict['{} c_frequency {}'.format(text,i)].items()}\n","    \n","    if norm:\n","        freqs_df = pd.DataFrame(norm_dict)\n","    else:\n","        freqs_df = pd.DataFrame(freq_dict)\n","    freqs_df = freqs_df.fillna(0)\n","    \n","    \n","    return freqs_df"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOGtb5xUaII_","executionInfo":{"status":"ok","timestamp":1624477947202,"user_tz":-120,"elapsed":8,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Returns a dataframe with the mean frequency of each word across different corpora.\n","# Input: frequency dataframe\n","def mean_freqs(freqs_df):\n","    return(freqs_df.mean(axis=1))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYpKr6JWaII_","executionInfo":{"status":"ok","timestamp":1624477947203,"user_tz":-120,"elapsed":6,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Returns a dataframe of word ranks for list of corpora,\n","# with each column corresponding to a different corpus.\n","# Input: list of corpora. Each corpus consists of a list of tokenized sentences.\n","def calculate_ranks(rank_sents, norm=False, text=None):\n","    ranks_dicts = {}\n","    for i, corpus in enumerate(rank_sents):\n","        freqs = collections.Counter(corpus)\n","        if norm:\n","            len_corp = len(corpus)\n","            for key in freqs:\n","                freqs[key] /= len_corp\n","        ranks_dicts['{} c_rank {}'.format(text, i)] = {w: r for r, (w, c) in enumerate(freqs.most_common(), 1)}\n","    \n","    ranks_df = pd.DataFrame(ranks_dicts)\n","    for column in ranks_df:\n","        min_rank = int(np.ceil(ranks_df[column].max() + 1))\n","        nan_rows = ranks_df[ranks_df[column].isnull()]\n","        num_nans = len(nan_rows)\n","        nan_ranks = list(range(min_rank, min_rank+num_nans))\n","        random.shuffle(nan_ranks)\n","        ranks_df.loc[ranks_df[column].isnull(), column] = nan_ranks\n","\n","    return ranks_df"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"PqLpC1afaIJA","executionInfo":{"status":"ok","timestamp":1624477949450,"user_tz":-120,"elapsed":201,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Returns a dataframe with the mean rank of each word across different corpora.\n","# Input: rank dataframe\n","def mean_ranks(ranks_df):\n","    return ranks_df.mean(axis=1)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_CelGehaIJB","executionInfo":{"status":"ok","timestamp":1624477949647,"user_tz":-120,"elapsed":5,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Creates combined dataframe of ranks and frequencies\n","# Input: 2 lists (freq_sents and rank_sents) of corpora. Each corpus\n","# consists of a list of tokenized sentences. These lists are to be obtained form\n","# subsampling.\n","def ranks_freqs(freq_sents, rank_sents, text=None, norm=False):\n","    freqs_df = calculate_freqs(freq_sents, text=text, norm=norm)\n","    freqs_df['Frequency'] = mean_freqs(freqs_df)\n","    ranks_df = calculate_ranks(rank_sents, text=text, norm=norm)\n","    ranks_df['Rank'] = mean_ranks(ranks_df)\n","    \n","    # Put mean ranks and freqs together and remove all words that\n","    # do not have both a rank and frequency (which happens when a word)\n","    # is only present in freq_sents and not in rank_sents or vice versa\n","    ranks_freqs_df = pd.concat([ranks_df, freqs_df], axis = 1)\n","    ranks_freqs_df = ranks_freqs_df.dropna()\n","#     ranks_freqs_df = ranks_freqs_df.loc[ranks_freqs_df['Frequency'] >=1]\n","    return ranks_freqs_df"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovcfClbNaIJC","executionInfo":{"status":"ok","timestamp":1624477950170,"user_tz":-120,"elapsed":204,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# MLE of Zipf's law parameters (alpha and beta)\n","class Mandelbrot(GenericLikelihoodModel):\n","\n","    def __init__(self, frequencies, ranks, **kwargs):\n","        if not len(frequencies) == len(ranks):\n","            raise ValueError(\"NOT THE SAME NUMBER OF RANKS AND FREQS!\")\n","        \n","        frequencies = np.asarray(frequencies)\n","        ranks = np.asarray(ranks)\n","        \n","        self.n_obs = np.sum(frequencies)\n","        \n","        super().__init__(endog=frequencies, exog=ranks, **kwargs)\n","        self.fit_result = None\n","    \n","\n","    def prob(self, params, ranks=None, log=False):\n","        if ranks is None:\n","            ranks = self.exog\n","        \n","        alpha, beta = params\n","        if log:\n","            return -alpha*lg(beta+ranks) - lg(zeta(alpha, q=beta+1.))\n","        else:\n","            return ((beta + ranks)**(-alpha))/zeta(alpha, q=beta+1.)\n","    \n","    \n","    def loglike(self, params):\n","        rs = self.exog\n","        fs = self.endog\n","        alpha, beta = params\n","        \n","#        if alpha > 10 or beta > 20:\n","#            return -np.inf\n","        \n","#         if alpha < 1.0 or beta < 0.0:\n","#             return -np.inf\n","        \n","        # no need to calculate P(r) when observed f(r) was zero\n","        log_probs = -alpha*lg(beta+rs) - lg(zeta(alpha, q=beta+1.))\n","        log_probs = log_probs.reshape(-1, )\n","        return np.sum(fs * log_probs) - beta**5\n","    \n","    \n","    def register_fit(self, fit_result, overwrite=False):\n","        if not self.fit_result is None and not overwrite:\n","            raise ValueError(\"A fit result is already registered and overwrite=False!\")\n","            \n","        self.fit_result = fit_result\n","        self.optim_params = fit_result.params\n","        self.pseudo_r_squared = self.pseudo_r_squared(self.optim_params)\n","        self.SE, self.SE_relative = fit_result.bse, fit_result.bse/self.optim_params\n","        self.BIC, self.BIC_relative = fit_result.bic,\\\n","                            (-2*self.null_loglike())/fit_result.bic\n","        \n","        return self.optim_params\n","    \n","    def print_result(self, string=False):\n","        if self.fit_result is None:\n","            raise ValueError(\"Register a fitting result first!\")\n","\n","        def format_x(x):\n","            return float('{0:.3g}'.format(x))\n","\n","\n","        s = \"=\"*50\n","        s += \"\\n\" + \"MANDELBROT\"\n","        s += \"\\n\" + \"  Optimal Parameters \" + str(tuple(map(format_x, self.optim_params)))\n","        \n","        s += \"\\n\" + \"  Standard Error [relative]: \" + str(tuple(map(format_x, self.SE))) +\\\n","              \", [\" + str(tuple(map(format_x, self.SE_relative))) + \"]\"\n","        \n","        s += \"\\n\" + \"  Pseudo R^2: \" + str(format_x(self.pseudo_r_squared))\n","        \n","        s += \"\\n\" + \"  BIC [relative]: \" + str(format_x(self.BIC)) +\\\n","              \", [\" + str(format_x(self.BIC_relative)) + \"]\"\n","        s += \"\\n\" + \"=\"*50\n","        \n","        if string:\n","            return s\n","        \n","        print(s)\n","    \n","    \n","    def null_loglike(self, epsilon=1e-10):\n","        return self.loglike((1.+epsilon, 0.0))\n","    \n","    def pseudo_r_squared(self, params):\n","        return 1-self.loglike(params)/self.null_loglike()\n","    \n","    \n","    def predict(self, params, ranks=None, freqs=True, n_obs=None, \n","                correct_for_finite_domain=True):\n","        if ranks is None:\n","            ranks = self.exog\n","        ranks = np.asarray(ranks)\n","        \n","        if n_obs is None:\n","            n_obs = self.n_obs\n","            \n","        alpha, beta = params\n","        pred_probs = self.prob(params, ranks=ranks, log=False)\n","        \n","        if correct_for_finite_domain:\n","            if not freqs:\n","                raise NotImplementedError(\"Correction for \"\\\n","                                          \"finite domain not implemented with probabilities!\")\n","            return pred_probs*(n_obs/np.sum(pred_probs))\n","        \n","        if freqs:\n","            return n_obs*pred_probs\n","        \n","        return pred_probs"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"UuLUgp0eaIJD","executionInfo":{"status":"ok","timestamp":1624477951963,"user_tz":-120,"elapsed":448,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Returns a dataframe containing the mean frequencies and ranks, as well as \n","# the estimated frequencies from Zipf's law and the error between the (log) mean\n","# frequencies and (log) estimated frequencies.\n","def zipfs_law(df, print_stats = True):\n","    mandelbrot = Mandelbrot(df['Frequency'], df['Rank'])\n","    mandelbrot_fit = mandelbrot.fit(start_params=np.asarray([1.0, 1.0]), # [1.0, 1.0]\n","                                method=\"powell\", full_output=True, disp=0)\n","    mandelbrot.register_fit(mandelbrot_fit)\n","    if print_stats:\n","        mandelbrot.print_result()\n","    \n","    model_params = mandelbrot.optim_params\n","    alpha, beta =  mandelbrot.optim_params\n","    preds = mandelbrot.predict(model_params, df['Rank'])\n","\n","    df['Estimated frequency'] = preds\n","    return df, [alpha, beta]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"4y5pbv8vaIJD","executionInfo":{"status":"ok","timestamp":1624477952163,"user_tz":-120,"elapsed":4,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["def plot_zipf(ranks_freqs_df):\n","    ranks_freqs_df = ranks_freqs_df.sort_values(by=['Rank'])\n","    zipf_df, params = zipfs_law(ranks_freqs_df)\n","#     ranks_freqs_df = ranks_freqs_df.loc[ranks_freqs_df['Frequency'] >=1]\n","#     hexbin_plot(ranks_freqs_df['Rank'], ranks_freqs_df['Frequency'], est = ranks_freqs_df['Estimated frequency'])\n","#     plt.show()\n","#     hexbin_error(zipf_df['Rank (log)'], zipf_df['Error'])\n","#     plt.show()\n","    \n","    return zipf_df"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xha7kzsfaIJF","executionInfo":{"status":"ok","timestamp":1624479051672,"user_tz":-120,"elapsed":183,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Divides a big corpus into \"n\" subcorpera and calculates the frequencies for each\n","# subcorpus. Returns a dataframe containing the frequencies by word and by rank.\n","def sample_corpora(corpus, text, n=10, norm=True, subclasses=False, pos=True,size=10):\n","    corpus = [item for sublist in corpus for item in sublist]\n","    rank_corp, freq_corp = subsampling(corpus, k=size*100, m=n)\n","\n","    if len(rank_corp[0]) == 0:\n","      print(corpus)\n","\n","    by_rank = pd.DataFrame()\n","    by_word = pd.DataFrame()\n","\n","    ranks_freqs_df = ranks_freqs(rank_corp, freq_corp, text=text, norm=norm)\n","    ranks_freqs_df, params = zipfs_law(ranks_freqs_df, print_stats=False)\n","    \n","    return  params\n","#     return None"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q7om3IvaIJF","executionInfo":{"status":"ok","timestamp":1624479065386,"user_tz":-120,"elapsed":254,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Takes 2 corpora and aligns their frequency values by specific words and ranks \n","# so that the Mann-Whitney test can be applied to the frequencies of every word\n","# or rank.\n","def mann_whitney_df(corpus, human, gpt, size, n=10, t=0, norm=True, subclasses=False):\n","    params_c = sample_corpora(corpus, text=\"C1\", n=n, norm=norm, subclasses=subclasses,size=size)\n","    params_h = sample_corpora(human, text=\"C2\", n=n, norm=norm, subclasses=subclasses,size=size)\n","    params_g = sample_corpora(gpt, text=\"C2\", n=n, norm=norm, subclasses=subclasses,size=size)\n","    \n","    \n","    dist_h = np.linalg.norm(np.array(params_c)-np.array(params_h))\n","    dist_g = np.linalg.norm(np.array(params_c)-np.array(params_g))\n","    \n","    return [dist_h, dist_g] "],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwNvxIgGaIJM","executionInfo":{"status":"ok","timestamp":1624479123486,"user_tz":-120,"elapsed":237,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["# Takes 2 corpora, and applies the Mann-Whitney procedure to \"times\" subparts\n","# of both corpora. \n","# Returns dataframes containing distributions of the total percentages as well \n","# as per-rank percentages of rejected H0 ranks and words.\n","def stats_dist2(corpus, human, gpt, times=10, n=10, t=0, norm=True, subclasses=False, pos=False):\n","    len_corp = int(len(human)/10)\n","    dists = []\n","    \n","    for i in range(times):\n","        corpus_samp = corpus\n","        human_samp = human[i*len_corp:(i+1)*len_corp]\n","        gpt_samp = gpt[i*len_corp:(i+1)*len_corp]\n","        dist = mann_whitney_df(corpus, human_samp, gpt_samp,size=len_corp, n=n, t=t, norm=norm, subclasses=subclasses)\n","        dists.append(dist)\n","   \n","    return dists"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"ErlrqhujaIJN","executionInfo":{"status":"ok","timestamp":1624479111776,"user_tz":-120,"elapsed":4,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}}},"source":["def test(corpus, size, rep = 1, times = 10, n=10, sub=True):\n","  \n","    dist_human = 0\n","    dist_machine = 0\n","\n","    corpus = corpus\n","    human = human_set\n","    random.shuffle(human)\n","    gpt = gpt_set\n","    random.shuffle(gpt)\n","    dists = stats_dist2(corpus, human[0:size*times], gpt[0:size*times], times=times, n=n)\n","\n","    if dists[0] < dists[1]:\n","      print(\"corpus is human\")\n","    else:\n","      print(\"corpus is gpt-2\")\n","\n","    return None"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOZN5_6-fYPq","executionInfo":{"status":"ok","timestamp":1624479178252,"user_tz":-120,"elapsed":1023,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}},"outputId":"d724f83c-e940-455a-8a35-29908a0b9a34"},"source":["test(gpt_set[1], 1)"],"execution_count":84,"outputs":[{"output_type":"stream","text":["corpus is gpt-2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrSj7arMcVdo","executionInfo":{"status":"ok","timestamp":1624479168339,"user_tz":-120,"elapsed":2340,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}},"outputId":"432e7ef6-88e4-4107-c4fc-d92745419b57"},"source":["test(gpt_set[10:20], 10)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["corpus is gpt-2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jxvyvsUfgTwi","executionInfo":{"status":"ok","timestamp":1624479202584,"user_tz":-120,"elapsed":998,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}},"outputId":"e2ba42a2-6ecb-4708-bfbe-8c5a4ac8628e"},"source":["test(human_set[1], 1)"],"execution_count":85,"outputs":[{"output_type":"stream","text":["corpus is human\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tw1ot8xx32NO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624479216778,"user_tz":-120,"elapsed":2761,"user":{"displayName":"Rachel van 't Hull","photoUrl":"","userId":"09062029537718222996"}},"outputId":"90f62f4f-ba24-43da-b4f2-ae84d2781c0a"},"source":["test(human_set[0:10], 10)"],"execution_count":86,"outputs":[{"output_type":"stream","text":["corpus is human\n"],"name":"stdout"}]}]}