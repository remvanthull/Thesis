{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import json\n",
    "from nltk import tokenize\n",
    "import collections\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel,\\\n",
    "        GenericLikelihoodModelResults\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from scipy.special import zeta\n",
    "from scipy.stats import binom\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lg = np.log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_train = pd.read_csv (r'data/webtext.train.csv')\n",
    "web_train = web_train.loc[web_train['length']>= 1000]\n",
    "web_train = web_train.dropna()\n",
    "web_train = web_train['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_train = pd.read_csv (r'data/small-117M-k40.train.csv')\n",
    "gpt_train = gpt_train.loc[gpt_train['length']>= 1000]\n",
    "gpt_train = gpt_train.dropna()\n",
    "gpt_train = gpt_train['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_train_20000 = pickle.load(open(\"web_train_20000.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_train_20000 = pickle.load(open(\"gpt_train_20000.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_GPT = open(\"data/GPT_set.txt\", \"r\").read()\n",
    "set_GPT = set_GPT.split(\"</doc>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_set = [make_file(corpus, multi=False, pos=True) for corpus in set_GPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_human = open(\"data/GPT_set.txt\", \"r\").read()\n",
    "set_human = set_human.split(\"</doc>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_set = [make_file(corpus, multi=False, pos=True) for corpus in set_human]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing without part of speech tags\n",
    "def remove_punctuation(text):\n",
    "    text = text.lower()\n",
    "    chars_to_remove = \"[\\n]!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    tr = str.maketrans(\" \", \" \", chars_to_remove)\n",
    "    return text.translate(tr)\n",
    "\n",
    "\n",
    "def preprocess(corpus, sent = True):\n",
    "    if sent:\n",
    "        corpus = tokenize.sent_tokenize(corpus)\n",
    "        corpus = [remove_punctuation(sent).split() for sent in corpus]\n",
    "    else:\n",
    "        corpus = remove_punctuation(corpus).split()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing with part of speech tags\n",
    "def part_of_speech(corpus):\n",
    "    corpus = tokenize.sent_tokenize(corpus)\n",
    "    chars_to_remove = \"[\\n]\"\n",
    "    tr = str.maketrans(\" \", \" \", chars_to_remove)\n",
    "    chars_to_remove2 = \"[\\n]!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    new_corp = []\n",
    "    test = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        sent = sent.translate(tr)\n",
    "        words_sent = tokenize.word_tokenize(sent)\n",
    "        sent_pos = nltk.pos_tag(words_sent)\n",
    "        new_sent = []\n",
    "        for (word, pos) in sent_pos:\n",
    "            tr2 = str.maketrans(\"\", \"\", chars_to_remove2)\n",
    "            word = word.translate(tr2)\n",
    "            if word:\n",
    "                new_sent.append((word.lower(), pos))\n",
    "        new_corp.append(new_sent)\n",
    "    return new_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total preprocessing function for a corpus. Input can be one string (corpus),\n",
    "# for which you put multi = False, or a list of several strings (corpora) that \n",
    "# you want to turn into one big corpus, for which you put multi = True.\n",
    "# For PoS tags, put pos = True.\n",
    "def make_file(corp, multi = True, sent = True, pos = False):\n",
    "    if multi:\n",
    "        corpus = ''\n",
    "        for subcorp in corp:\n",
    "            corpus += subcorp\n",
    "    else:\n",
    "        corpus = corp\n",
    "        \n",
    "    if pos:\n",
    "        corpus = part_of_speech(corpus)\n",
    "    \n",
    "    else:\n",
    "        corpus = preprocess(corpus, sent = sent)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns 2 lists of corpora, one from which the ranks will be calculated\n",
    "# and one from which the frequencies will be calculated. Each corpus consists of\n",
    "# a list of tokenized sentences.\n",
    "# Input: corpus that is to be subsampled. Should be a list of tokenized sentences.\n",
    "# k is the amount of tokens that each sampled corpus should contain,\n",
    "# m is the amount of subcorpera you want for both the ranks and frequencies.\n",
    "# Max: I would read Valentin's thesis for an explanation on subsampling\n",
    "def subsampling(corpus, k = 1000000, m = 10, sent = True):\n",
    "    n = len(corpus)\n",
    "    \n",
    "    sen_len = {}\n",
    "\n",
    "    \n",
    "    rank_corpera = []\n",
    "    freq_corpera = []\n",
    "\n",
    "    if sent:\n",
    "        for i in range(m):\n",
    "            used_rank = set()\n",
    "            used_freq = set()\n",
    "            rank_count = 0\n",
    "            freq_count = 0\n",
    "            rank_samples = []\n",
    "            freq_samples = []\n",
    "\n",
    "            while rank_count < k:\n",
    "                index = np.random.randint(n)\n",
    "                if index in used_rank:\n",
    "                    continue\n",
    "\n",
    "                rank_sample = corpus[index]\n",
    "                len_sample = len(rank_sample)\n",
    "\n",
    "                if len_sample == 0:\n",
    "                    continue\n",
    "\n",
    "                if rank_count > k:\n",
    "                    max_len = len_sample - (rank_count - k)\n",
    "                    rank_sample = rank_sample[:max_len]\n",
    "                    \n",
    "                rank_samples += rank_sample\n",
    "                rank_count += len_sample\n",
    "\n",
    "\n",
    "                used_rank.add(index)\n",
    "\n",
    "            while freq_count < k:\n",
    "                index = np.random.randint(n)\n",
    "                if index in used_freq:\n",
    "                    continue\n",
    "                freq_sample = corpus[index]\n",
    "                len_sample = len(freq_sample)\n",
    "\n",
    "                if len_sample == 0:\n",
    "                    continue\n",
    "                    \n",
    "                if freq_count > k:\n",
    "                    max_len = len_sample - (freq_count - k)\n",
    "                    freq_sample = freq_sample[:max_len]\n",
    "\n",
    "                freq_samples += freq_sample\n",
    "                freq_count += len_sample\n",
    "\n",
    "                if len_sample not in sen_len and len_sample < 200:\n",
    "                    sen_len[len_sample] = 1\n",
    "                elif len_sample < 200:\n",
    "                    sen_len[len_sample] += 1\n",
    "\n",
    "                used_freq.add(index)\n",
    "\n",
    "            rank_corpera.append(rank_samples)\n",
    "            freq_corpera.append(freq_samples)\n",
    "#                 rank_corpera.append([item for sublist in rank_samples for item in sublist])\n",
    "#                 freq_corpera.append([item for sublist in freq_samples for item in sublist])\n",
    "\n",
    "\n",
    "    else:\n",
    "        for i in range(m):\n",
    "            rank_samples = random.sample(corpus, k)\n",
    "            freq_samples = random.sample(corpus, k)\n",
    "            rank_corpera.append(rank_samples)\n",
    "            freq_corpera.append(freq_samples)\n",
    "    \n",
    "#     return rank_corpera, freq_corpera, sen_len\n",
    "    return rank_corpera, freq_corpera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-Frequency calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe of word frequencies for list of corpora,\n",
    "# with each column corresponding to a different corpus.\n",
    "# Input: list of corpora. Each corpus consists of a list of tokenized sentences.\n",
    "def calculate_freqs(freq_sents, norm=True, text=None):\n",
    "    freq_dict = {}\n",
    "    norm_dict = {}\n",
    "    for i, corpus in enumerate(freq_sents):\n",
    "        freq_dict['{} c_frequency {}'.format(text,i)] = collections.Counter(corpus)\n",
    "        if norm:\n",
    "            len_corp = len(corpus)\n",
    "            norm_dict['{} c_frequency {}'.format(text, i)] = {k: v / len_corp for k, v in freq_dict['{} c_frequency {}'.format(text,i)].items()}\n",
    "    \n",
    "    if norm:\n",
    "        freqs_df = pd.DataFrame(norm_dict)\n",
    "    else:\n",
    "        freqs_df = pd.DataFrame(freq_dict)\n",
    "    freqs_df = freqs_df.fillna(0)\n",
    "    \n",
    "    \n",
    "    return freqs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe with the mean frequency of each word across different corpora.\n",
    "# Input: frequency dataframe\n",
    "def mean_freqs(freqs_df):\n",
    "    return(freqs_df.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe of word ranks for list of corpora,\n",
    "# with each column corresponding to a different corpus.\n",
    "# Input: list of corpora. Each corpus consists of a list of tokenized sentences.\n",
    "def calculate_ranks(rank_sents, norm=False, text=None):\n",
    "    ranks_dicts = {}\n",
    "    for i, corpus in enumerate(rank_sents):\n",
    "        freqs = collections.Counter(corpus)\n",
    "        if norm:\n",
    "            len_corp = len(corpus)\n",
    "            for key in freqs:\n",
    "                freqs[key] /= len_corp\n",
    "        ranks_dicts['{} c_rank {}'.format(text, i)] = {w: r for r, (w, c) in enumerate(freqs.most_common(), 1)}\n",
    "    \n",
    "    ranks_df = pd.DataFrame(ranks_dicts)\n",
    "    for column in ranks_df:\n",
    "        min_rank = int(np.ceil(ranks_df[column].max() + 1))\n",
    "        nan_rows = ranks_df[ranks_df[column].isnull()]\n",
    "        num_nans = len(nan_rows)\n",
    "        nan_ranks = list(range(min_rank, min_rank+num_nans))\n",
    "        random.shuffle(nan_ranks)\n",
    "        ranks_df.loc[ranks_df[column].isnull(), column] = nan_ranks\n",
    "\n",
    "    return ranks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe with the mean rank of each word across different corpora.\n",
    "# Input: rank dataframe\n",
    "def mean_ranks(ranks_df):\n",
    "    return ranks_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates combined dataframe of ranks and frequencies\n",
    "# Input: 2 lists (freq_sents and rank_sents) of corpora. Each corpus\n",
    "# consists of a list of tokenized sentences. These lists are to be obtained form\n",
    "# subsampling.\n",
    "def ranks_freqs(freq_sents, rank_sents, text=None, norm=False):\n",
    "    freqs_df = calculate_freqs(freq_sents, text=text, norm=norm)\n",
    "    freqs_df['Frequency'] = mean_freqs(freqs_df)\n",
    "    ranks_df = calculate_ranks(rank_sents, text=text, norm=norm)\n",
    "    ranks_df['Rank'] = mean_ranks(ranks_df)\n",
    "    \n",
    "    # Put mean ranks and freqs together and remove all words that\n",
    "    # do not have both a rank and frequency (which happens when a word)\n",
    "    # is only present in freq_sents and not in rank_sents or vice versa\n",
    "    ranks_freqs_df = pd.concat([ranks_df, freqs_df], axis = 1)\n",
    "    ranks_freqs_df = ranks_freqs_df.dropna()\n",
    "#     ranks_freqs_df = ranks_freqs_df.loc[ranks_freqs_df['Frequency'] >=1]\n",
    "    return ranks_freqs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE of Zipf's law parameters (alpha and beta)\n",
    "class Mandelbrot(GenericLikelihoodModel):\n",
    "\n",
    "    def __init__(self, frequencies, ranks, **kwargs):\n",
    "        if not len(frequencies) == len(ranks):\n",
    "            raise ValueError(\"NOT THE SAME NUMBER OF RANKS AND FREQS!\")\n",
    "        \n",
    "        frequencies = np.asarray(frequencies)\n",
    "        ranks = np.asarray(ranks)\n",
    "        \n",
    "        self.n_obs = np.sum(frequencies)\n",
    "        \n",
    "        super().__init__(endog=frequencies, exog=ranks, **kwargs)\n",
    "        self.fit_result = None\n",
    "    \n",
    "\n",
    "    def prob(self, params, ranks=None, log=False):\n",
    "        if ranks is None:\n",
    "            ranks = self.exog\n",
    "        \n",
    "        alpha, beta = params\n",
    "        if log:\n",
    "            return -alpha*lg(beta+ranks) - lg(zeta(alpha, q=beta+1.))\n",
    "        else:\n",
    "            return ((beta + ranks)**(-alpha))/zeta(alpha, q=beta+1.)\n",
    "    \n",
    "    \n",
    "    def loglike(self, params):\n",
    "        rs = self.exog\n",
    "        fs = self.endog\n",
    "        alpha, beta = params\n",
    "        \n",
    "#        if alpha > 10 or beta > 20:\n",
    "#            return -np.inf\n",
    "        \n",
    "#         if alpha < 1.0 or beta < 0.0:\n",
    "#             return -np.inf\n",
    "        \n",
    "        # no need to calculate P(r) when observed f(r) was zero\n",
    "        log_probs = -alpha*lg(beta+rs) - lg(zeta(alpha, q=beta+1.))\n",
    "        log_probs = log_probs.reshape(-1, )\n",
    "        return np.sum(fs * log_probs) - beta**5\n",
    "    \n",
    "    \n",
    "    def register_fit(self, fit_result, overwrite=False):\n",
    "        if not self.fit_result is None and not overwrite:\n",
    "            raise ValueError(\"A fit result is already registered and overwrite=False!\")\n",
    "            \n",
    "        self.fit_result = fit_result\n",
    "        self.optim_params = fit_result.params\n",
    "        self.pseudo_r_squared = self.pseudo_r_squared(self.optim_params)\n",
    "        self.SE, self.SE_relative = fit_result.bse, fit_result.bse/self.optim_params\n",
    "        self.BIC, self.BIC_relative = fit_result.bic,\\\n",
    "                            (-2*self.null_loglike())/fit_result.bic\n",
    "        \n",
    "        return self.optim_params\n",
    "    \n",
    "    def print_result(self, string=False):\n",
    "        if self.fit_result is None:\n",
    "            raise ValueError(\"Register a fitting result first!\")\n",
    "\n",
    "        def format_x(x):\n",
    "            return float('{0:.3g}'.format(x))\n",
    "\n",
    "\n",
    "        s = \"=\"*50\n",
    "        s += \"\\n\" + \"MANDELBROT\"\n",
    "        s += \"\\n\" + \"  Optimal Parameters \" + str(tuple(map(format_x, self.optim_params)))\n",
    "        \n",
    "        s += \"\\n\" + \"  Standard Error [relative]: \" + str(tuple(map(format_x, self.SE))) +\\\n",
    "              \", [\" + str(tuple(map(format_x, self.SE_relative))) + \"]\"\n",
    "        \n",
    "        s += \"\\n\" + \"  Pseudo R^2: \" + str(format_x(self.pseudo_r_squared))\n",
    "        \n",
    "        s += \"\\n\" + \"  BIC [relative]: \" + str(format_x(self.BIC)) +\\\n",
    "              \", [\" + str(format_x(self.BIC_relative)) + \"]\"\n",
    "        s += \"\\n\" + \"=\"*50\n",
    "        \n",
    "        if string:\n",
    "            return s\n",
    "        \n",
    "        print(s)\n",
    "    \n",
    "    \n",
    "    def null_loglike(self, epsilon=1e-10):\n",
    "        return self.loglike((1.+epsilon, 0.0))\n",
    "    \n",
    "    def pseudo_r_squared(self, params):\n",
    "        return 1-self.loglike(params)/self.null_loglike()\n",
    "    \n",
    "    \n",
    "    def predict(self, params, ranks=None, freqs=True, n_obs=None, \n",
    "                correct_for_finite_domain=True):\n",
    "        if ranks is None:\n",
    "            ranks = self.exog\n",
    "        ranks = np.asarray(ranks)\n",
    "        \n",
    "        if n_obs is None:\n",
    "            n_obs = self.n_obs\n",
    "            \n",
    "        alpha, beta = params\n",
    "        pred_probs = self.prob(params, ranks=ranks, log=False)\n",
    "        \n",
    "        if correct_for_finite_domain:\n",
    "            if not freqs:\n",
    "                raise NotImplementedError(\"Correction for \"\\\n",
    "                                          \"finite domain not implemented with probabilities!\")\n",
    "            return pred_probs*(n_obs/np.sum(pred_probs))\n",
    "        \n",
    "        if freqs:\n",
    "            return n_obs*pred_probs\n",
    "        \n",
    "        return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe containing the mean frequencies and ranks, as well as \n",
    "# the estimated frequencies from Zipf's law and the error between the (log) mean\n",
    "# frequencies and (log) estimated frequencies.\n",
    "def zipfs_law(df, print_stats = True):\n",
    "    mandelbrot = Mandelbrot(df['Frequency'], df['Rank'])\n",
    "    mandelbrot_fit = mandelbrot.fit(start_params=np.asarray([1.0, 1.0]), # [1.0, 1.0]\n",
    "                                method=\"powell\", full_output=True, disp=0)\n",
    "    mandelbrot.register_fit(mandelbrot_fit)\n",
    "    if print_stats:\n",
    "        mandelbrot.print_result()\n",
    "    \n",
    "    model_params = mandelbrot.optim_params\n",
    "    alpha, beta =  mandelbrot.optim_params\n",
    "    preds = mandelbrot.predict(model_params, df['Rank'])\n",
    "\n",
    "    df['Estimated frequency'] = preds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zipf(ranks_freqs_df):\n",
    "    ranks_freqs_df = ranks_freqs_df.sort_values(by=['Rank'])\n",
    "    zipf_df = zipfs_law(ranks_freqs_df)\n",
    "#     ranks_freqs_df = ranks_freqs_df.loc[ranks_freqs_df['Frequency'] >=1]\n",
    "#     hexbin_plot(ranks_freqs_df['Rank'], ranks_freqs_df['Frequency'], est = ranks_freqs_df['Estimated frequency'])\n",
    "#     plt.show()\n",
    "#     hexbin_error(zipf_df['Rank (log)'], zipf_df['Error'])\n",
    "#     plt.show()\n",
    "    \n",
    "    return zipf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divides a big corpus into \"n\" subcorpera and calculates the frequencies for each\n",
    "# subcorpus. Returns a dataframe containing the frequencies by word and by rank.\n",
    "def sample_corpora(corpus, text, n=10, norm=True, subclasses=False):\n",
    "    num_corp = len(corpus)\n",
    "    corpus = [item for sublist in corpus for item in sublist]\n",
    "    rank_corp, freq_corp = subsampling(corpus, k=num_corp*100, m=n)\n",
    "\n",
    "    by_rank = pd.DataFrame()\n",
    "    by_word = pd.DataFrame()\n",
    "\n",
    "    ranks_freqs_df = ranks_freqs(rank_corp, freq_corp, text=text, norm=norm)\n",
    "#     print(ranks_freqs_df)\n",
    "    ranks_freqs_df = zipfs_law(ranks_freqs_df, print_stats=False)\n",
    "    ranks_freqs_df['Error'] = abs(ranks_freqs_df['Frequency'] - ranks_freqs_df['Estimated frequency'])\n",
    "    ranks_freqs_df['Tot rank'] = ranks_freqs_df['Rank'].rank(method='first')\n",
    "    \n",
    "    by_ranks_pos = ranks_freqs_df.copy()\n",
    "    by_ranks_pos.reset_index(inplace=True)\n",
    "\n",
    "    if subclasses == False:\n",
    "        by_ranks_pos['level_1'] = by_ranks_pos['level_1'].replace({'NN': 'Noun', 'NNS':'Noun', \n",
    "                                           'NNP':'Noun', 'VB':'Verb', 'VBD':'Verb', \n",
    "                                           'VBG':'Verb', 'VBN':'Verb', 'VBP':'Verb', \n",
    "                                          'VBZ':'Verb', 'JJ':'Adjective', \n",
    "                                           'JJR':'Adjective', 'JJS':'Adjective', 'RB':'Adverb',\n",
    "                                          'RBR':'Adverb', 'RBS':'Adverb'})\n",
    "    \n",
    "\n",
    "    by_ranks_pos['PoS rank'] = by_ranks_pos.groupby('level_1')['Rank'].rank(method='first')\n",
    "    by_ranks_pos = by_ranks_pos.set_index(['level_1', 'PoS rank'])\n",
    "    \n",
    "    if subclasses == False:\n",
    "        classes = ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
    "    else:\n",
    "        classes = ['NN', 'NNS', 'NNP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    \n",
    "    by_ranks_pos = by_ranks_pos.loc[classes]\n",
    "    by_ranks_pos = by_ranks_pos.filter(regex='c_frequency|Frequency|Error')\n",
    "    by_ranks_pos = by_ranks_pos.rename(columns={\"Frequency\": \"{} mean freq\".format(text),\n",
    "                                         \"Error\": \"{} error\".format(text)})\n",
    "    \n",
    "\n",
    "    \n",
    "    by_rank = ranks_freqs_df.filter(regex='Tot rank|c_frequency|Frequency|Error').set_index(['Tot rank'])\n",
    "    by_rank = by_rank.rename(columns={\"Frequency\": \"{} mean freq\".format(text),\n",
    "                                         \"Error\": \"{} error\".format(text)})\n",
    "    \n",
    "    by_word = ranks_freqs_df.filter(regex='c_frequency|Frequency|Error')\n",
    "    by_word = by_word.rename(columns={\"Frequency\": \"{} mean freq\".format(text),\n",
    "                                         \"Error\": \"{} error\".format(text)})\n",
    "\n",
    "    by_word = by_word.sort_values(by=['{} mean freq'.format(text)], ascending=False)\n",
    "    by_rank = by_rank.sort_values(by=['Tot rank'])\n",
    "    by_ranks_pos = by_ranks_pos.sort_index(by=['level_1', 'PoS rank'])\n",
    "    \n",
    "    by_word = by_word.fillna(0)\n",
    "    by_rank = by_rank.dropna()\n",
    "    by_ranks_pos = by_ranks_pos.dropna()\n",
    "    \n",
    "    return by_word, by_rank, by_ranks_pos\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 corpora and aligns their frequency values by specific words and ranks \n",
    "# so that the Mann-Whitney test can be applied to the frequencies of every word\n",
    "# or rank.\n",
    "def mann_whitney_df(corpus1, corpus2, n=10, t=0, norm=True, subclasses=False):\n",
    "    words_corpus1, ranks_corpus1, ranks_pos_corpus1 = sample_corpora(corpus1, text=\"C1\", n=n, norm=norm, subclasses=subclasses)\n",
    "    words_corpus2, ranks_corpus2, ranks_pos_corpus2 = sample_corpora(corpus2, text=\"C2\", n=n, norm=norm, subclasses=subclasses)\n",
    "    \n",
    "    words_df = pd.concat([words_corpus1, words_corpus2], axis=1)\n",
    "    lol = words_df.loc[:, words_df.columns.str.contains('freq')].fillna(0)\n",
    "    words_df.loc[:, words_df.columns.str.contains('freq')] = lol\n",
    "    words_df['error diff'] = abs(words_df['C1 error'] - words_df['C2 error'])\n",
    "#     words_df = words_df.fillna(0)\n",
    "    \n",
    "    \n",
    "    ranks_df = pd.concat([ranks_corpus1, ranks_corpus2], axis=1)\n",
    "    ranks_df = ranks_df.dropna()\n",
    "    ranks_df['error diff'] = abs(ranks_df['C1 error'] - ranks_df['C2 error'])\n",
    "\n",
    "    ranks_pos_df = pd.concat([ranks_pos_corpus1, ranks_pos_corpus2], axis=1)\n",
    "    ranks_pos_df = ranks_pos_df.dropna()\n",
    "    return words_df, ranks_df, ranks_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Mann-Whitney test to a dataframe containing frequencies per word\n",
    "# or rank.\n",
    "def mann_whitney_test(df, n=10):\n",
    "    stats = []\n",
    "    p = []\n",
    "    \n",
    "    df1 = df.loc[:, df.columns.str.contains('frequency')]\n",
    "    \n",
    "    for index, row in df1.iterrows():\n",
    "        mann = mannwhitneyu(row[0:n], row[n:])\n",
    "        stats.append(mann[0])\n",
    "        p.append(mann[1])\n",
    "    df['statistics'] = stats\n",
    "    df['p-value'] = p\n",
    "    df[\"H0\"] = df['p-value'] > 0.05\n",
    "    df = df.filter(regex='mean freq|error|fano|statistics|p-value|H0')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 corpora, and applies the Mann-Whitney test to ranks and specific words.\n",
    "# Returns dataframes containing the results for both methods.\n",
    "def mann_whitney_words_ranks(corpus1, corpus2, n=10, repeat = 10, t=0, norm=True, subclasses=False):\n",
    "    words_df, ranks_df, ranks_pos_df = mann_whitney_df(corpus1, corpus2, n=n, t=t, norm=norm, subclasses=subclasses)\n",
    "    df1 = mann_whitney_test(words_df, n=n)\n",
    "    df2 = mann_whitney_test(ranks_df, n=n)\n",
    "    df3 = mann_whitney_test(ranks_pos_df, n=n)\n",
    "    return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 corpora, and applies the Mann-Whitney test to specific words and ranks.\n",
    "# Calculates for both methods the percentage of words/ranks that reject H0.\n",
    "def stats_mw(corpus1, corpus2, n=10, t=0, norm=True, print_stats=True, subclasses=False):\n",
    "    words_df, ranks_df, ranks_pos_df = mann_whitney_words_ranks(corpus1, corpus2, n=n, t=t, norm=norm, subclasses=subclasses)\n",
    "\n",
    "    tot_words = len(words_df)\n",
    "    no_h0_words = len(words_df.loc[words_df['p-value']<=0.05])\n",
    "    perc_words = no_h0_words/tot_words*100\n",
    "    \n",
    "    tot_ranks = len(ranks_df)\n",
    "    no_h0_ranks = len(ranks_df.loc[ranks_df['p-value']<=0.05])\n",
    "    perc_ranks = no_h0_ranks/tot_ranks*100\n",
    "    \n",
    "    if print_stats:\n",
    "        print(\"WORDS:\\n\")\n",
    "        print(\"Total words: \", tot_words)\n",
    "        print(\"No H0: \", no_h0_words)\n",
    "        print(\"Percentage: \", perc_words)\n",
    "    \n",
    "        print(\"\\n\\nRANKS:\\n\")\n",
    "        print(\"Total ranks: \", tot_ranks)\n",
    "        print(\"No H0: \", no_h0_ranks)\n",
    "        print(\"Percentage: \", perc_ranks)\n",
    "    \n",
    "    stats = [perc_words, perc_ranks]\n",
    "    \n",
    "    return words_df, ranks_df, stats, ranks_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a dataframe with calculated Mann-Whitney values (of 2 corpora) for words \n",
    "# or ranks.\n",
    "# Returns: dataframe with percentages of words or ranks that reject H0, grouped by\n",
    "# ranks.\n",
    "def dif_ranks(df, error=True):\n",
    "    df_by_ranks = pd.Series()\n",
    "    error_by_ranks = pd.Series()\n",
    "    \n",
    "    ranks = df[0:10]\n",
    "    \n",
    "    try:\n",
    "        top_ten = len(ranks.loc[ranks['p-value']<=0.05]) * 10\n",
    "    except ZeroDivisionError:\n",
    "        top_ten = None\n",
    "    df_by_ranks['1-10'] = top_ten\n",
    "    \n",
    "    if error:\n",
    "        error_ten = ranks['error diff'].mean()\n",
    "        error_by_ranks['1-10'] = error_ten\n",
    "    cur_ranks = 10\n",
    "    max_rank = len(df)\n",
    "    \n",
    "    while 2*cur_ranks <= max_rank:\n",
    "        ranks = df[cur_ranks:2*cur_ranks]\n",
    "        try:\n",
    "            perc = len(ranks.loc[ranks['p-value']<=0.05])/len(ranks) * 100\n",
    "        except ZeroDivisionError:\n",
    "            perc = None\n",
    "        df_by_ranks['{}-{}'.format(cur_ranks+1,2*cur_ranks)] = perc\n",
    "        if error:\n",
    "            error1 = ranks['error diff'].mean()\n",
    "            error_by_ranks['{}-{}'.format(cur_ranks+1,2*cur_ranks)] = error1\n",
    "        cur_ranks *= 2\n",
    "        \n",
    "    ranks = df[cur_ranks:]\n",
    "    try:\n",
    "        perc = len(ranks.loc[ranks['p-value']<=0.05])/len(ranks) * 100\n",
    "    except ZeroDivisionError:\n",
    "        perc = None\n",
    "    df_by_ranks['{}-end'.format(cur_ranks+1,max_rank)] = perc\n",
    "    \n",
    "    if error:\n",
    "        error1 = ranks['error diff'].mean()\n",
    "        error_by_ranks['{}-end'.format(cur_ranks+1,2*cur_ranks)] = error1\n",
    "        return df_by_ranks, error_by_ranks\n",
    "    \n",
    "    return df_by_ranks.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 corpora, and applies the Mann-Whitney procedure to \"times\" subparts\n",
    "# of both corpora. \n",
    "# Returns dataframes containing distributions of the total percentages as well \n",
    "# as per-rank percentages of rejected H0 ranks and words.\n",
    "def stats_dist(corpus1, corpus2, times=10, n=10, t=0, norm=True, subclasses=False):\n",
    "    if subclasses == False:\n",
    "        classes = ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
    "    else:\n",
    "        classes = ['NN', 'NNS', 'NNP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    \n",
    "    len_corp = int(len(corpus1)/times)\n",
    "    ranks_stats_df = {}\n",
    "    words_stats_df = {}\n",
    "    \n",
    "    pos_stats_df = pd.DataFrame()\n",
    "    \n",
    "    error_ranks_stats_df = {}\n",
    "    error_words_stats_df = {}\n",
    "    \n",
    "    \n",
    "    for i in range(times):\n",
    "        i_words = {}\n",
    "        i_ranks = {}\n",
    "        \n",
    "        i_error_words = {}\n",
    "        i_error_ranks = {}\n",
    "        \n",
    "        corpus1_samp = corpus1[i*len_corp:(i+1)*len_corp]\n",
    "        corpus2_samp = corpus2[i*len_corp:(i+1)*len_corp]\n",
    "        words_df, ranks_df, stats, ranks_pos_df = stats_mw(corpus1_samp, corpus2_samp, n=n, t=t, norm=norm, print_stats=False, subclasses=subclasses)\n",
    "        words_df['comb mean'] = words_df[['C1 mean freq', 'C2 mean freq']].mean(axis=1)\n",
    "        words_df = words_df.sort_values(by=['comb mean'], ascending=False)\n",
    "        ranks_df = ranks_df.sort_values(by=['Tot rank'])\n",
    "        \n",
    "        \n",
    "        dif_ranks_words, dif_ranks_error_words = dif_ranks(words_df)\n",
    "        dif_ranks_ranks, dif_ranks_error_ranks = dif_ranks(ranks_df)\n",
    "        dif_ranks_pos = dif_ranks(ranks_pos_df, error=False)\n",
    "        \n",
    "        \n",
    "        i_words['total']= stats[0]\n",
    "        for index, value in dif_ranks_words.items():\n",
    "            i_words[index] = value\n",
    "        \n",
    "        i_ranks['total'] = stats[1]\n",
    "        for index, value in dif_ranks_ranks.items():\n",
    "            i_ranks[index] = value\n",
    "         \n",
    "        \n",
    "    \n",
    "        i_pos = pd.DataFrame()\n",
    "        pos = {}\n",
    "        for pos_class in classes:\n",
    "            pos[i] = {}\n",
    "            df = ranks_pos_df.loc[[pos_class]]\n",
    "            try:\n",
    "                pos[i][(pos_class, 'total')] = len(df.loc[df['p-value']<=0.05])/len(df) * 100\n",
    "            except ZeroDivisionError:\n",
    "                pos[i][(pos_class,'total')] = None\n",
    "            dif_ranks_pos = dif_ranks(ranks_pos_df.loc[[pos_class]], error=False).sort_index()\n",
    "            for index, value in dif_ranks_pos.items():\n",
    "                pos[i][(pos_class,index)] = value\n",
    "            i_pos = pd.concat([i_pos, pd.DataFrame(pos)], axis=0)\n",
    "            \n",
    "        \n",
    "\n",
    "        pos_stats_df = pd.concat([i_pos, pos_stats_df], axis=1).sort_index()\n",
    "        words_stats_df[i] = i_words\n",
    "        ranks_stats_df[i] = i_ranks\n",
    "        \n",
    "#         print(ranks_df.loc[ranks_df['H0'] == False]['error diff'])\n",
    "        \n",
    "        i_error_words['total'] = words_df['error diff'].mean()\n",
    "        for index, value in dif_ranks_error_words.items():\n",
    "            i_error_words[index] = value\n",
    "        \n",
    "        i_error_ranks['total'] = ranks_df['error diff'].mean()\n",
    "        for index, value in dif_ranks_error_ranks.items():\n",
    "            i_error_ranks[index] = value\n",
    "            \n",
    "        error_words_stats_df[i] = i_error_words\n",
    "        error_ranks_stats_df[i] = i_error_ranks\n",
    "        \n",
    "   \n",
    "    return pd.DataFrame(words_stats_df), pd.DataFrame(ranks_stats_df), pos_stats_df, pd.DataFrame(error_words_stats_df), pd.DataFrame(error_ranks_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readable_stats(corpus1, corpus2, times=10, n=10, t=0, norm=True, subclasses=False):\n",
    "    words_stats_df, ranks_stats_df, pos_stats_df, words_error_stats_df, ranks_error_stats_df = stats_dist(corpus1, corpus2, times=times, n=n, t=t, norm=norm, subclasses=subclasses)\n",
    "    ranks = pd.DataFrame()\n",
    "    words = pd.DataFrame()\n",
    "    pos = pd.DataFrame()\n",
    "    \n",
    "    words['mean perc'] = words_stats_df.mean(axis=1)\n",
    "    words['std perc'] = words_stats_df.std(axis=1)\n",
    "#     words['mean error'] = words_error_stats_df.mean(axis=1)\n",
    "#     words['std error'] = words_error_stats_df.std(axis=1)\n",
    "\n",
    "    \n",
    "    ranks['mean perc'] = ranks_stats_df.mean(axis=1)\n",
    "    ranks['std perc'] = ranks_stats_df.std(axis=1)\n",
    "#     ranks['mean error'] = ranks_error_stats_df.mean(axis=1)\n",
    "#     ranks['std error'] = ranks_error_stats_df.std(axis=1)\n",
    "    \n",
    "    pos['mean perc'] = pos_stats_df.mean(axis = 1)\n",
    "    pos['std perc'] = pos_stats_df.std(axis = 1)\n",
    "    \n",
    "#     return words, ranks, pos, words_stats_df, ranks_stats_df, pos_stats_df, words_error_stats_df, ranks_error_stats_df\n",
    "\n",
    "\n",
    "    return words, ranks, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tests(size, start=0, times=10, n=10):\n",
    "    words_gpt_web, ranks_gpt_web, pos_gpt_web = readable_stats(gpt_train_20000[start:start+size*times], web_train_20000[start:start+size*times], times=times, n=n, t=0, norm=True, subclasses=False)\n",
    "    words_web_web, ranks_web_web, pos_web_web = readable_stats(web_train_20000[start:start+size*times], web_train_20000[start+size*times:start+2*size*times], times=times, n=n, t=0, norm=True, subclasses=False)\n",
    "    words_gpt_gpt, ranks_gpt_gpt, pos_gpt_gpt = readable_stats(gpt_train_20000[start:start+size*times], gpt_train_20000[start+size*times:start+2*size*times], times=times, n=n, t=0, norm=True, subclasses=False)\n",
    "    \n",
    "    words = pd.concat([words_gpt_web, words_web_web, words_gpt_gpt], axis=1)\n",
    "    ranks = pd.concat([ranks_gpt_web, ranks_web_web, ranks_gpt_gpt], axis=1)\n",
    "    pos =  pd.concat([pos_gpt_web, pos_web_web, pos_gpt_gpt], axis=1)\n",
    "    return words, ranks, pos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 corpora, and applies the Mann-Whitney procedure to \"times\" subparts\n",
    "# of both corpora. \n",
    "# Returns dataframes containing distributions of the total percentages as well \n",
    "# as per-rank percentages of rejected H0 ranks and words.\n",
    "def stats_dist2(corpus1, corpus2, times=10, n=10, t=0, norm=True, subclasses=False):\n",
    "    if subclasses == False:\n",
    "        classes = ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
    "    else:\n",
    "        classes = ['NN', 'NNS', 'NNP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    \n",
    "    len_corp = int(len(corpus2)/times)\n",
    "    ranks_stats_df = {}\n",
    "    words_stats_df = {}\n",
    "    \n",
    "    pos_stats_df = pd.DataFrame()\n",
    "    \n",
    "    error_ranks_stats_df = {}\n",
    "    error_words_stats_df = {}\n",
    "    \n",
    "    \n",
    "    for i in range(times):\n",
    "        i_words = {}\n",
    "        i_ranks = {}\n",
    "        \n",
    "        i_error_words = {}\n",
    "        i_error_ranks = {}\n",
    "        \n",
    "        corpus1_samp = corpus1\n",
    "        corpus2_samp = corpus2[i*len_corp:(i+1)*len_corp]\n",
    "        words_df, ranks_df, stats, ranks_pos_df = stats_mw(corpus1_samp, corpus2_samp, n=n, t=t, norm=norm, print_stats=False, subclasses=subclasses)\n",
    "        words_df['comb mean'] = words_df[['C1 mean freq', 'C2 mean freq']].mean(axis=1)\n",
    "        words_df = words_df.sort_values(by=['comb mean'], ascending=False)\n",
    "        ranks_df = ranks_df.sort_values(by=['Tot rank'])\n",
    "        \n",
    "        \n",
    "        dif_ranks_words, dif_ranks_error_words = dif_ranks(words_df)\n",
    "        dif_ranks_ranks, dif_ranks_error_ranks = dif_ranks(ranks_df)\n",
    "        dif_ranks_pos = dif_ranks(ranks_pos_df, error=False)\n",
    "        \n",
    "        \n",
    "        i_words['total']= stats[0]\n",
    "        for index, value in dif_ranks_words.items():\n",
    "            i_words[index] = value\n",
    "        \n",
    "        i_ranks['total'] = stats[1]\n",
    "        for index, value in dif_ranks_ranks.items():\n",
    "            i_ranks[index] = value\n",
    "         \n",
    "        \n",
    "    \n",
    "        i_pos = pd.DataFrame()\n",
    "        pos = {}\n",
    "        for pos_class in classes:\n",
    "            pos[i] = {}\n",
    "            df = ranks_pos_df.loc[[pos_class]]\n",
    "            try:\n",
    "                pos[i][(pos_class, 'total')] = len(df.loc[df['p-value']<=0.05])/len(df) * 100\n",
    "            except ZeroDivisionError:\n",
    "                pos[i][(pos_class,'total')] = None\n",
    "            dif_ranks_pos = dif_ranks(ranks_pos_df.loc[[pos_class]], error=False).sort_index()\n",
    "            for index, value in dif_ranks_pos.items():\n",
    "                pos[i][(pos_class,index)] = value\n",
    "            i_pos = pd.concat([i_pos, pd.DataFrame(pos)], axis=0)\n",
    "            \n",
    "        \n",
    "\n",
    "        pos_stats_df = pd.concat([i_pos, pos_stats_df], axis=1).sort_index()\n",
    "        words_stats_df[i] = i_words\n",
    "        ranks_stats_df[i] = i_ranks\n",
    "        \n",
    "#         print(ranks_df.loc[ranks_df['H0'] == False]['error diff'])\n",
    "        \n",
    "        i_error_words['total'] = words_df['error diff'].mean()\n",
    "        for index, value in dif_ranks_error_words.items():\n",
    "            i_error_words[index] = value\n",
    "        \n",
    "        i_error_ranks['total'] = ranks_df['error diff'].mean()\n",
    "        for index, value in dif_ranks_error_ranks.items():\n",
    "            i_error_ranks[index] = value\n",
    "            \n",
    "        error_words_stats_df[i] = i_error_words\n",
    "        error_ranks_stats_df[i] = i_error_ranks\n",
    "        \n",
    "   \n",
    "    return pd.DataFrame(words_stats_df), pd.DataFrame(ranks_stats_df), pos_stats_df, pd.DataFrame(error_words_stats_df), pd.DataFrame(error_ranks_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readable_stats2(corpus1, corpus2, times=10, n=10, t=0, norm=True, subclasses=False):\n",
    "    words_stats_df, ranks_stats_df, pos_stats_df, words_error_stats_df, ranks_error_stats_df = stats_dist2(corpus1, corpus2, times=times, n=n, t=t, norm=norm, subclasses=subclasses)\n",
    "    ranks = pd.DataFrame()\n",
    "    words = pd.DataFrame()\n",
    "    pos = pd.DataFrame()\n",
    "    \n",
    "    words['mean perc'] = words_stats_df.mean(axis=1)\n",
    "    ranks['mean perc'] = ranks_stats_df.mean(axis=1)\n",
    "    \n",
    "    pos['mean perc'] = pos_stats_df.mean(axis = 1)\n",
    "    \n",
    "    return words, ranks, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion that automatically classifes corpora as being human or machine, using\n",
    "# the Mann-Whitney test based on the top 60 ranks of individual words and ranks.\n",
    "#\n",
    "# Parameters:\n",
    "# - gpt: True when you're classifying gpt-generated corpus, false when\n",
    "# you're classifying a human-written corpus.\n",
    "# \n",
    "# - size: amount of texts you want to classify at a time (I normally use 10 \n",
    "# so that you have about 10 x 1.000 = 10.000 tokens)\n",
    "# \n",
    "# - own_set: When none, it will use the webtext and gpt set, or you can enter\n",
    "# Riemer's sets which it will then use.\n",
    "# \n",
    "# - rep: amount of repetitions, so how many corpora of size \"size\" you want to \n",
    "# classify\n",
    "# \n",
    "# - times = how many times you want the mann-whitney test to be repeated on a corpus\n",
    "# \n",
    "# - n: mann-whitney parameter, namely the amount of subcorpora you want to divide the\n",
    "# input into\n",
    "\n",
    "# returns: the amount of times a text got classified as human and as machine-generated\n",
    "# for both the use of individual words and ranks, alongside a list of the mean \n",
    "# percentages for these outcomes.\n",
    "\n",
    "    def test_rep(gpt, size, own_set=None, rep = 50, times = 5, n=10):\n",
    "    j = size\n",
    "    \n",
    "    num_corp = size * times\n",
    "    \n",
    "    w_human = 0\n",
    "    w_machine = 0\n",
    "    r_human = 0\n",
    "    r_machine = 0\n",
    "    \n",
    "    means_w_human = []\n",
    "    means_w_machine =[]\n",
    "    means_r_human = []\n",
    "    means_r_machine = []\n",
    "\n",
    "    indexen = [0, 14, 24, 40, 53, 65]\n",
    "    \n",
    "    for i in range(rep):\n",
    "        random.shuffle(gpt_train_20000)\n",
    "        random.shuffle(web_train_20000)\n",
    "        \n",
    "        if gpt == True:\n",
    "            if own_set is not None:\n",
    "                corpus = own_set[indexen[i]:indexen[i+1]]\n",
    "            else:\n",
    "                corpus = gpt_train_20000[:size]\n",
    "        else:\n",
    "            if own_set is not None:\n",
    "                corpus = own_set[indexen[i]:indexen[i+1]]\n",
    "            else:\n",
    "                corpus = web_train_20000[:size]\n",
    "        \n",
    "        web = web_train_20000[size:size+num_corp]\n",
    "        gpt = gpt_train_20000[size:size+num_corp]\n",
    "        words_web, ranks_web, pos_web = readable_stats2(corpus, web, times=times, n=n)\n",
    "        words_gpt, ranks_gpt, pos_gpt = readable_stats2(corpus, gpt, times=times, n=n)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        if words_web['mean perc'][1:5].mean() < words_gpt['mean perc'][1:5].mean():\n",
    "            w_human += 1\n",
    "        \n",
    "        else:\n",
    "            w_machine += 1\n",
    "        means_w_machine.append(words_gpt['mean perc'][1:5].mean())\n",
    "        \n",
    "        if ranks_web['mean perc'][1:5].mean() < ranks_gpt['mean perc'][1:5].mean():\n",
    "            r_human += 1\n",
    "        else:\n",
    "            r_machine += 1\n",
    "        \n",
    "        means_w_human.append(words_web['mean perc'][1:5].mean())\n",
    "        means_w_machine.append(words_gpt['mean perc'][1:5].mean())\n",
    "        means_r_human.append(ranks_web['mean perc'][1:5].mean())\n",
    "        means_r_machine.append(ranks_gpt['mean perc'][1:5].mean())\n",
    "        \n",
    "    means_list = [np.mean(means_w_human), np.mean(means_r_human), np.mean(means_w_machine), np.mean(means_r_machine)]\n",
    "    return w_human, r_human, w_machine, r_machine, means_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voor Riemer:\n",
    "test_rep is de functie die automatisch teksten classificeert; hierbij krijg je niet de mega tabellen te zien waarin de Mann-Whitney test weergeven staat, maar\n",
    "slechts de resultaten. Stel je wil maar 1 van je generated corpussen checken, van bijv 10 teksten, vul je dus bijv in ```test_rep(True, 10, own_set=jouw_set[0:10], rep = 1, times = 10, n=10)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 0, 4, 5, [59.825, 55.725, 50.01042656530461, 35.81152993348115])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rep(False, 10, own_set=True, rep = 5, times = 10, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[item for sublist in corpus for item in sublist] for corpus in human_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(corpus) for corpus in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5721"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lens[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = [[item for sublist in corpus for item in sublist] for corpus in gpt_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens2 = [len(corpus) for corpus in test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10033"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lens[54:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv (r'data/webtext.test.csv')\n",
    "test = test.loc[test['length']>= 1000]\n",
    "test = test.dropna()\n",
    "test = test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv (r'data/small-117M-k40.test.csv')\n",
    "test = test.loc[test['length']>= 1000]\n",
    "test = test.dropna()\n",
    "test = test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>ended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255001</td>\n",
       "      <td>Dawn on its way to victory, but where does it ...</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255002</td>\n",
       "      <td>\\nDrew Angerer/Getty Images\\n\\nThere are many ...</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255003</td>\n",
       "      <td>In my last post, I gave a post on the use of t...</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255004</td>\n",
       "      <td>Brought to you by:\\n\\nKFC\\n\\nHogan's Day\\n\\nSo...</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255005</td>\n",
       "      <td>\\nIt's time for the first time since 2008, and...</td>\n",
       "      <td>480</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>259996</td>\n",
       "      <td>I've read articles talking about how to manage...</td>\n",
       "      <td>111</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>259997</td>\n",
       "      <td>The first part of my work-in-progress, The Unf...</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>259998</td>\n",
       "      <td>A lot of these things are already pretty cool,...</td>\n",
       "      <td>395</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>259999</td>\n",
       "      <td>S. 3148  114th Congress (2017-2018) To amend ...</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>260000</td>\n",
       "      <td>Founded in 1991, the Texas Instruments family ...</td>\n",
       "      <td>232</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  length  ended\n",
       "0     255001  Dawn on its way to victory, but where does it ...    1024  False\n",
       "1     255002  \\nDrew Angerer/Getty Images\\n\\nThere are many ...    1024  False\n",
       "2     255003  In my last post, I gave a post on the use of t...    1024  False\n",
       "3     255004  Brought to you by:\\n\\nKFC\\n\\nHogan's Day\\n\\nSo...    1024  False\n",
       "4     255005  \\nIt's time for the first time since 2008, and...     480   True\n",
       "...      ...                                                ...     ...    ...\n",
       "4995  259996  I've read articles talking about how to manage...     111   True\n",
       "4996  259997  The first part of my work-in-progress, The Unf...    1024  False\n",
       "4997  259998  A lot of these things are already pretty cool,...     395   True\n",
       "4998  259999  S. 3148  114th Congress (2017-2018) To amend ...    1024  False\n",
       "4999  260000  Founded in 1991, the Texas Instruments family ...     232   True\n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv (r'data/small-117M-k40.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_web = [make_file(text, multi=False, pos=True) for text in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt = [make_file(text, multi=False, pos=True) for text in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_gpt, open( \"test_gpt.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_1 = pickle.load(open(\"df_real_1.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pickle.load(open(\"web_train_20000.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(colour, text):\n",
    "    if colour == \"red\":\n",
    "        return \"\\033[1;41m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"green\":\n",
    "        return \"\\033[1;42m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"yellow\":\n",
    "        return \"\\033[1;43m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"cyan\":\n",
    "        return \"\\033[1;46m\" + str(text) + \"\\033[1;m\"\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-675a6cbb2388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "[0,0,0].add([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_text(corpus):\n",
    "    text = \"\"\n",
    "    count = [0,0,0,0,0,0]\n",
    "    for word, pos in corpus:\n",
    "        try:\n",
    "            rank = df_reals.loc[word]['Tot rank'][0]\n",
    "            if rank <= 10:\n",
    "                text += highlight(\"green\", word)\n",
    "                count[0] += 1\n",
    "            elif rank <= 100:\n",
    "                text += highlight(\"yellow\", word)\n",
    "                count[1] += 1\n",
    "            elif rank <= 1000:\n",
    "                text += highlight(\"red\", word)\n",
    "                count[2] += 1\n",
    "            elif rank <= 10000:\n",
    "                text += highlight(\"cyan\", word)\n",
    "                count[3] += 1\n",
    "            else:\n",
    "                text += word\n",
    "                count[4] += 1\n",
    "\n",
    "        except KeyError:\n",
    "            text += highlight(\"magenta\", word)\n",
    "            count[5] += 1\n",
    "        text += \" \"\n",
    "    count = [0 for num in count if not num]\n",
    "    count = [num/len(corpus) for num in count]\n",
    "    return text, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [[item for sublist in corpus for item in sublist] for corpus in test_gpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "loll = [len(corpus) for corpus in lol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = [0,0,0,0,0,0]\n",
    "for corpus in test_web:\n",
    "    corpus = [item for sublist in corpus for item in sublist]\n",
    "    if len(corpus) == 0:\n",
    "        continue\n",
    "    count = color_text(corpus)[1]\n",
    "    tot = [x + y for x, y in zip(tot, count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_gpt = [0,0,0,0,0,0]\n",
    "for corpus in test_gpt:\n",
    "    corpus = [item for sublist in corpus for item in sublist]\n",
    "    if len(corpus) == 0:\n",
    "        continue\n",
    "    count = color_text(corpus)[1]\n",
    "    tot_gpt = [x + y for x, y in zip(tot, count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[160.08554996405465,\n",
       " 165.39611790079078,\n",
       " 157.84974838245867,\n",
       " 175.60388209920922,\n",
       " 76.72969086987779,\n",
       " 29.74550682961898]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num/len(test_web) for num in tot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[133.48532055122828,\n",
       " 137.86339125224686,\n",
       " 131.64230077890952,\n",
       " 146.46434991012583,\n",
       " 63.98382264829239,\n",
       " 24.829239065308567]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num/len(test_gpt) for num in tot_gpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_1 = color_text(test_gpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dawn \u001b[1;42mon\u001b[1;m \u001b[1;43mits\u001b[1;m \u001b[1;41mway\u001b[1;m \u001b[1;42mto\u001b[1;m victory \u001b[1;43mbut\u001b[1;m \u001b[1;43mwhere\u001b[1;m \u001b[1;41mdoes\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;41mend\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mbeen\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mlast\u001b[1;m \u001b[1;41mfive\u001b[1;m \u001b[1;41mdays\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mlast\u001b[1;m \u001b[1;41mday\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41moctober\u001b[1;m \u001b[1;43mi\u001b[1;m \u001b[1;41mthought\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mwere\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mlot\u001b[1;m \u001b[1;42mof\u001b[1;m announcements \u001b[1;43mfrom\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m makers \u001b[1;43mover\u001b[1;m \u001b[1;43mat\u001b[1;m eurogamer \u001b[1;42mand\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mfact\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mwill\u001b[1;m \u001b[1;43monly\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;43mone\u001b[1;m announcement \u001b[1;43mat\u001b[1;m \u001b[1;42mthe\u001b[1;m close \u001b[1;43mwas\u001b[1;m \u001b[1;41mgreat\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43mwas\u001b[1;m \u001b[1;43malso\u001b[1;m \u001b[1;42mthe\u001b[1;m beginning \u001b[1;42mof\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mreally\u001b[1;m \u001b[1;41mlong\u001b[1;m \u001b[1;41mday\u001b[1;m \u001b[1;42mof\u001b[1;m speculation \u001b[1;42mand\u001b[1;m discussion \u001b[1;43mabout\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mend\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mwhat\u001b[1;m s \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;41mdone\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;43mwe\u001b[1;m d \u001b[1;41mneed\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mlot\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mhelp\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mmain\u001b[1;m \u001b[1;41mreason\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mare\u001b[1;m nt \u001b[1;43many\u001b[1;m dlc \u001b[1;42mfor\u001b[1;m dragon \u001b[1;41mage\u001b[1;m \u001b[1;41m2\u001b[1;m \u001b[1;43mso\u001b[1;m \u001b[1;41mfar\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;42mthe\u001b[1;m studio \u001b[1;43mhas\u001b[1;m \u001b[1;42ma\u001b[1;m ton \u001b[1;42mof\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;41mthings\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mdo\u001b[1;m \u001b[1;43mfrom\u001b[1;m \u001b[1;43mwhat\u001b[1;m \u001b[1;43mwas\u001b[1;m \u001b[1;41mexpected\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;43mthem\u001b[1;m \u001b[1;42mto\u001b[1;m dlc \u001b[1;43mwe\u001b[1;m \u001b[1;43malso\u001b[1;m \u001b[1;43mhave\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mbiggest\u001b[1;m \u001b[1;41mdevelopment\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;41mdevelopment\u001b[1;m \u001b[1;41mteam\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mhelp\u001b[1;m \u001b[1;41mus\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43mfrom\u001b[1;m \u001b[1;41mgame\u001b[1;m designer \u001b[1;42mto\u001b[1;m writer \u001b[1;43mthey\u001b[1;m \u001b[1;43mhave\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mlot\u001b[1;m \u001b[1;42mof\u001b[1;m ideas \u001b[1;43mabout\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mwork\u001b[1;m \u001b[1;41maround\u001b[1;m \u001b[1;43msome\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mmore\u001b[1;m complicated dlcs \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;43mthey\u001b[1;m ve \u001b[1;43mhad\u001b[1;m \u001b[1;43msome\u001b[1;m \u001b[1;41mreally\u001b[1;m interesting discussions \u001b[1;43mwith\u001b[1;m \u001b[1;41mus\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m studio \u001b[1;41mincluding\u001b[1;m \u001b[1;41mtalking\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;41mwhether\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mbring\u001b[1;m dlc \u001b[1;43mlike\u001b[1;m \u001b[1;41mdeath\u001b[1;m knight dragon \u001b[1;41mage\u001b[1;m \u001b[1;41m2\u001b[1;m \u001b[1;43mor\u001b[1;m \u001b[1;43meven\u001b[1;m shadow \u001b[1;42mof\u001b[1;m mordor \u001b[1;41mthrough\u001b[1;m \u001b[1;43mas\u001b[1;m \u001b[1;43mthey\u001b[1;m re \u001b[1;43mjust\u001b[1;m \u001b[1;41mdoing\u001b[1;m \u001b[1;42mthe\u001b[1;m dlc \u001b[1;43mwhich\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;41msee\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m trailer \u001b[1;41mwhether\u001b[1;m \u001b[1;43mor\u001b[1;m \u001b[1;43mnot\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mdo\u001b[1;m \u001b[1;43mthem\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m alpha \u001b[1;43mthen\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mfirst\u001b[1;m \u001b[1;41mstep\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mlooking\u001b[1;m \u001b[1;43minto\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mdetails\u001b[1;m \u001b[1;42mfor\u001b[1;m \u001b[1;43mwhich\u001b[1;m dlc \u001b[1;43mwill\u001b[1;m \u001b[1;43mbe\u001b[1;m included \u001b[1;43mwhich\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mhuge\u001b[1;m priority \u001b[1;42mfor\u001b[1;m \u001b[1;43mthose\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mwho\u001b[1;m \u001b[1;43mdo\u001b[1;m nt \u001b[1;41mknow\u001b[1;m \u001b[1;43mwhat\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;42mis\u001b[1;m essentially \u001b[1;42ma\u001b[1;m turnbased tactical combat \u001b[1;41mgame\u001b[1;m \u001b[1;43mwhere\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;42mthe\u001b[1;m commander \u001b[1;42mof\u001b[1;m \u001b[1;43man\u001b[1;m army \u001b[1;42mand\u001b[1;m \u001b[1;43myou\u001b[1;m fight \u001b[1;41magainst\u001b[1;m \u001b[1;41manother\u001b[1;m \u001b[1;41mplayer\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mwhole\u001b[1;m premise \u001b[1;42mof\u001b[1;m \u001b[1;43mthis\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mwhy\u001b[1;m \u001b[1;43mwe\u001b[1;m re \u001b[1;41mhere\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthen\u001b[1;m \u001b[1;41meach\u001b[1;m turn \u001b[1;43myou\u001b[1;m lose \u001b[1;43myou\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mone\u001b[1;m \u001b[1;43mwho\u001b[1;m loses \u001b[1;43mbecause\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;41mlost\u001b[1;m \u001b[1;42mand\u001b[1;m therefore \u001b[1;43mnot\u001b[1;m necessarily \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43mwhere\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;41mable\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mfind\u001b[1;m \u001b[1;41mthings\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mdo\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mwhere\u001b[1;m \u001b[1;43mtheir\u001b[1;m \u001b[1;41mplans\u001b[1;m \u001b[1;43mwill\u001b[1;m fall \u001b[1;42mand\u001b[1;m \u001b[1;43mthen\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mwill\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;43mmore\u001b[1;m \u001b[1;41mdevelopment\u001b[1;m \u001b[1;43mwith\u001b[1;m \u001b[1;41mus\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43meven\u001b[1;m \u001b[1;43mtheir\u001b[1;m \u001b[1;41mown\u001b[1;m \u001b[1;41mdevelopment\u001b[1;m \u001b[1;41mteam\u001b[1;m \u001b[1;43mthey\u001b[1;m ll \u001b[1;43mget\u001b[1;m \u001b[1;41minvolved\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;43mwill\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mform\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m firstperson shooter \u001b[1;41mgame\u001b[1;m \u001b[1;43mwhich\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41msomething\u001b[1;m \u001b[1;43mwe\u001b[1;m \u001b[1;43mare\u001b[1;m excited \u001b[1;43mabout\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;43mwill\u001b[1;m \u001b[1;41mcontinue\u001b[1;m \u001b[1;42mto\u001b[1;m grow \u001b[1;43mas\u001b[1;m \u001b[1;43mthis\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;42mis\u001b[1;m developed \u001b[1;41mfurther\u001b[1;m \u001b[1;43minto\u001b[1;m \u001b[1;42mthe\u001b[1;m franchise \u001b[1;43mthere\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mlot\u001b[1;m \u001b[1;43mmore\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;42mthe\u001b[1;m creative \u001b[1;41mside\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41msomething\u001b[1;m \u001b[1;43mwe\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;41mreally\u001b[1;m excited \u001b[1;43mabout\u001b[1;m \u001b[1;43mbecause\u001b[1;m \u001b[1;42mit\u001b[1;m opens \u001b[1;43mup\u001b[1;m \u001b[1;43mmore\u001b[1;m opportunities \u001b[1;42mfor\u001b[1;m \u001b[1;41mfuture\u001b[1;m titles \u001b[1;42mit\u001b[1;m \u001b[1;43malso\u001b[1;m \u001b[1;43mhas\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mstory\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;43mwe\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mtell\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42ma\u001b[1;m sort \u001b[1;42mof\u001b[1;m fantasy \u001b[1;41mworld\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;41mplay\u001b[1;m \u001b[1;43mas\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisitor \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisition \u001b[1;42mand\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;41mknow\u001b[1;m \u001b[1;43myour\u001b[1;m \u001b[1;41mfamily\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthis\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mlittle\u001b[1;m \u001b[1;41mbit\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mfamily\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;41mdeal\u001b[1;m \u001b[1;43mwith\u001b[1;m \u001b[1;41mvarious\u001b[1;m evil \u001b[1;43mpeople\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;43mwill\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;41mset\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42ma\u001b[1;m fantasy \u001b[1;41mworld\u001b[1;m \u001b[1;43mwhere\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mhave\u001b[1;m \u001b[1;43mno\u001b[1;m \u001b[1;41mcontrol\u001b[1;m \u001b[1;43mover\u001b[1;m \u001b[1;43mthem\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mhave\u001b[1;m \u001b[1;42mto\u001b[1;m fight \u001b[1;43mthem\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisitor \u001b[1;42mis\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mget\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mfirst\u001b[1;m \u001b[1;41mman\u001b[1;m \u001b[1;42mto\u001b[1;m rescue \u001b[1;43mthem\u001b[1;m \u001b[1;43mfrom\u001b[1;m \u001b[1;43mthose\u001b[1;m evil gods \u001b[1;42mand\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41muse\u001b[1;m \u001b[1;43mtheir\u001b[1;m powers \u001b[1;42mto\u001b[1;m defeat \u001b[1;43mthese\u001b[1;m evil gods \u001b[1;43mso\u001b[1;m \u001b[1;43mwhat\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mstory\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mvery\u001b[1;m \u001b[1;41mstory\u001b[1;m \u001b[1;41mbased\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mare\u001b[1;m lots \u001b[1;42mof\u001b[1;m \u001b[1;41mthings\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43mwith\u001b[1;m \u001b[1;43msome\u001b[1;m \u001b[1;41mreally\u001b[1;m amazing writing \u001b[1;42mthe\u001b[1;m \u001b[1;41mstory\u001b[1;m \u001b[1;41mreally\u001b[1;m revolves \u001b[1;41maround\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisition \u001b[1;42mthe\u001b[1;m church \u001b[1;42mand\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;43mthey\u001b[1;m \u001b[1;41mgo\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;43mtheir\u001b[1;m missions \u001b[1;41mthrough\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mseries\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mevents\u001b[1;m \u001b[1;43mwhich\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mcan\u001b[1;m imagine \u001b[1;43mwould\u001b[1;m \u001b[1;41mbecome\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mworld\u001b[1;m \u001b[1;41mbuilding\u001b[1;m missions \u001b[1;41mcoming\u001b[1;m \u001b[1;43minto\u001b[1;m \u001b[1;41mplay\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mexactly\u001b[1;m \u001b[1;43mwhat\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mat\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mend\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mday\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43man\u001b[1;m exploration \u001b[1;41mgame\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mmuch\u001b[1;m deeper \u001b[1;42mand\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mnot\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;41mbeing\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;42min\u001b[1;m progress \u001b[1;43mbecause\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;43mnot\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisitor \u001b[1;43mor\u001b[1;m \u001b[1;42mthe\u001b[1;m bishop \u001b[1;43mor\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mbad\u001b[1;m guy \u001b[1;43mbecause\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisitor \u001b[1;42mis\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mhe\u001b[1;m \u001b[1;43mhas\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mdo\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mfor\u001b[1;m \u001b[1;43mthem\u001b[1;m \u001b[1;43myou\u001b[1;m re fighting \u001b[1;41magainst\u001b[1;m demons battling mages \u001b[1;43mthat\u001b[1;m sort \u001b[1;42mof\u001b[1;m \u001b[1;41mthing\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisition \u001b[1;42mis\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mcompletely\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;41mkind\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mjust\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;43mmore\u001b[1;m unique \u001b[1;41mway\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mtrying\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mbuild\u001b[1;m \u001b[1;42ma\u001b[1;m larger \u001b[1;41mgroup\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mcreate\u001b[1;m \u001b[1;43mnew\u001b[1;m \u001b[1;41mforces\u001b[1;m \u001b[1;42mto\u001b[1;m fight alongside \u001b[1;43mthat\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;43mwill\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;43mlike\u001b[1;m \u001b[1;43man\u001b[1;m mmorpg \u001b[1;43mwith\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mworld\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;43mhas\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m built \u001b[1;41maround\u001b[1;m \u001b[1;42mthe\u001b[1;m inquisition \u001b[1;43mas\u001b[1;m \u001b[1;41mwell\u001b[1;m \u001b[1;43mas\u001b[1;m \u001b[1;43msome\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;41mrules\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mcourse\u001b[1;m \u001b[1;43mwe\u001b[1;m \u001b[1;43mhave\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mlot\u001b[1;m \u001b[1;42mof\u001b[1;m ideas \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mworks\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mall\u001b[1;m \u001b[1;41mgreat\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;41msuch\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mgood\u001b[1;m \u001b[1;41mstory\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mwe\u001b[1;m love \u001b[1;41mplaying\u001b[1;m dragon \u001b[1;41mage\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42mit\u001b[1;m s definitely \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mvery\u001b[1;m fun \u001b[1;41mgame\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mplay\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mgreat\u001b[1;m \u001b[1;41mplace\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mput\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mworld\u001b[1;m \u001b[1;43mwhen\u001b[1;m \u001b[1;43mi\u001b[1;m ve \u001b[1;41masked\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mrole\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mits\u001b[1;m \u001b[1;41mpossible\u001b[1;m \u001b[1;41mrole\u001b[1;m \u001b[1;42min\u001b[1;m dragons \u001b[1;42mof\u001b[1;m skyrim \u001b[1;43mwhat\u001b[1;m s \u001b[1;42mthe\u001b[1;m \u001b[1;41mmost\u001b[1;m anticipated \u001b[1;41mmoment\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;41mtakes\u001b[1;m \u001b[1;41mplace\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;43mso\u001b[1;m \u001b[1;43mi\u001b[1;m \u001b[1;41masked\u001b[1;m \u001b[1;43mif\u001b[1;m \u001b[1;43mthere\u001b[1;m s \u001b[1;41msomething\u001b[1;m \u001b[1;43mthat\u001b[1;m s \u001b[1;41mcoming\u001b[1;m \u001b[1;43mthat\u001b[1;m s \u001b[1;41mvery\u001b[1;m \u001b[1;43mnew\u001b[1;m \u001b[1;41msomething\u001b[1;m \u001b[1;43mthat\u001b[1;m s \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;41mvery\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;43mthan\u001b[1;m \u001b[1;43mwhat\u001b[1;m \u001b[1;41mfans\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;41mcurrently\u001b[1;m \u001b[1;41mused\u001b[1;m \u001b[1;42mto\u001b[1;m watching \u001b[1;43mbut\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;41mvery\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;43mthan\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mrest\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mnot\u001b[1;m \u001b[1;42ma\u001b[1;m traditional rpg \u001b[1;42mit\u001b[1;m s \u001b[1;43mnot\u001b[1;m \u001b[1;43man\u001b[1;m \u001b[1;41maction\u001b[1;m rpg \u001b[1;42mthe\u001b[1;m \u001b[1;41mstory\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mnot\u001b[1;m \u001b[1;41mtrying\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mjust\u001b[1;m \u001b[1;41mtake\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43minto\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mworld\u001b[1;m \u001b[1;42mand\u001b[1;m fight \u001b[1;41moff\u001b[1;m demons \u001b[1;42mand\u001b[1;m demons \u001b[1;42mand\u001b[1;m demons \u001b[1;42mand\u001b[1;m \u001b[1;43mso\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mnot\u001b[1;m \u001b[1;41mtrying\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mmake\u001b[1;m \u001b[1;43mthis\u001b[1;m \u001b[1;43mjust\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mgame\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;41mrather\u001b[1;m \u001b[1;42ma\u001b[1;m fantasy \u001b[1;41mworld\u001b[1;m \u001b[1;41mbased\u001b[1;m \u001b[1;41maround\u001b[1;m dragons dragons \u001b[1;41mbeing\u001b[1;m dragon dragons \u001b[1;41mbeing\u001b[1;m dragons dragons \u001b[1;41mbeing\u001b[1;m dragons \u001b[1;42mand\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mme\u001b[1;m \u001b[1;43mthat\u001b[1;m s \u001b[1;43mwhat\u001b[1;m \u001b[1;41mreally\u001b[1;m \u001b[1;41mmade\u001b[1;m dragon \u001b[1;41mage\u001b[1;m \u001b[1;43mso\u001b[1;m \u001b[1;41mwell\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mso\u001b[1;m \u001b[1;41measy\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mare\u001b[1;m \u001b[1;43mso\u001b[1;m \u001b[1;43mmany\u001b[1;m \u001b[1;41mdifferent\u001b[1;m styles \u001b[1;42mand\u001b[1;m ideas \u001b[1;43mthat\u001b[1;m \u001b[1;43mwere\u001b[1;m \u001b[1;41mgoing\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m introduced \u001b[1;42min\u001b[1;m dragon \u001b[1;41mage\u001b[1;m \u001b[1;43mwhen\u001b[1;m \u001b[1;43mwe\u001b[1;m \u001b[1;43mwere\u001b[1;m \u001b[1;41mmaking\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;42mand\u001b[1;m \n"
     ]
    }
   ],
   "source": [
    "print(gpt_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_1 = color_text(test_web[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;42mthe\u001b[1;m wolf \u001b[1;41mcurrently\u001b[1;m \u001b[1;43mhas\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mformer\u001b[1;m seminole \u001b[1;43mas\u001b[1;m \u001b[1;43mhis\u001b[1;m rb33 \u001b[1;43mbut\u001b[1;m expect dalvin cook \u001b[1;42mto\u001b[1;m \u001b[1;43mget\u001b[1;m \u001b[1;42ma\u001b[1;m gigantic bump \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m rankings \u001b[1;43mafter\u001b[1;m \u001b[1;41mtaking\u001b[1;m advantage \u001b[1;42mof\u001b[1;m latavius murray s absence \u001b[1;41mduring\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mfirst\u001b[1;m \u001b[1;41mweek\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mtraining\u001b[1;m camp cook \u001b[1;42mis\u001b[1;m catching \u001b[1;42mthe\u001b[1;m eye \u001b[1;42mof\u001b[1;m \u001b[1;43mhis\u001b[1;m teammates \u001b[1;42mand\u001b[1;m coaches \u001b[1;42mand\u001b[1;m \u001b[1;41mcurrently\u001b[1;m possesses workhorse \u001b[1;41mpotential\u001b[1;m \u001b[1;42min\u001b[1;m minnesota \u001b[1;42ma\u001b[1;m succession \u001b[1;42mof\u001b[1;m poor decisions \u001b[1;41moff\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mfield\u001b[1;m ball \u001b[1;41msecurity\u001b[1;m \u001b[1;41missues\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mfield\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42ma\u001b[1;m mediocre combine dropped \u001b[1;42mthe\u001b[1;m ubertalented dalvin cook \u001b[1;43mall\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mway\u001b[1;m \u001b[1;42mto\u001b[1;m pick \u001b[1;43mno\u001b[1;m 41 \u001b[1;42mto\u001b[1;m \u001b[1;42mthe\u001b[1;m minnesota vikings  \u001b[1;43mwho\u001b[1;m \u001b[1;41mlet\u001b[1;m vikings legend \u001b[1;42mand\u001b[1;m seventime allpro adrian peterson walk \u001b[1;42min\u001b[1;m \u001b[1;41mfree\u001b[1;m \u001b[1;41magency\u001b[1;m \u001b[1;43mwhile\u001b[1;m \u001b[1;41madding\u001b[1;m \u001b[1;41mgoal\u001b[1;m \u001b[1;41mline\u001b[1;m hammer latavius murray \u001b[1;42min\u001b[1;m \u001b[1;41mmarch\u001b[1;m \u001b[1;43mafter\u001b[1;m christian mccaffrey \u001b[1;41mcame\u001b[1;m \u001b[1;41moff\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mboard\u001b[1;m \u001b[1;41mus\u001b[1;m \u001b[1;43mat\u001b[1;m \u001b[1;42mthe\u001b[1;m roto \u001b[1;41mstreet\u001b[1;m journal \u001b[1;43mwere\u001b[1;m pleading \u001b[1;42mfor\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mteam\u001b[1;m \u001b[1;43mlike\u001b[1;m \u001b[1;42mthe\u001b[1;m bucs raiders \u001b[1;43mor\u001b[1;m packers \u001b[1;42mto\u001b[1;m select cook \u001b[1;41monce\u001b[1;m \u001b[1;42mthe\u001b[1;m vikings drafted cook \u001b[1;42mthe\u001b[1;m fantasy \u001b[1;41mcommunity\u001b[1;m broke \u001b[1;43mout\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;42ma\u001b[1;m cold sweat minnesota \u001b[1;41mdoes\u001b[1;m \u001b[1;41mvery\u001b[1;m \u001b[1;41mlittle\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41mhelp\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43mtheir\u001b[1;m \u001b[1;41mrunning\u001b[1;m backs \u001b[1;43mfrom\u001b[1;m \u001b[1;43mtheir\u001b[1;m porous offensive \u001b[1;41mline\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mtheir\u001b[1;m upanddown passing \u001b[1;41msituation\u001b[1;m \u001b[1;42mand\u001b[1;m blahscheme \u001b[1;42mit\u001b[1;m s tough \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mrunning\u001b[1;m \u001b[1;41mback\u001b[1;m \u001b[1;42mfor\u001b[1;m \u001b[1;42mthe\u001b[1;m purple \u001b[1;42mand\u001b[1;m gold \u001b[1;42mthe\u001b[1;m wolf \u001b[1;43mwas\u001b[1;m \u001b[1;41mespecially\u001b[1;m hurt \u001b[1;43mby\u001b[1;m \u001b[1;43mthis\u001b[1;m combination stating cook landed \u001b[1;42min\u001b[1;m arguably \u001b[1;42mthe\u001b[1;m worst spot \u001b[1;41mpossible\u001b[1;m \u001b[1;43mwith\u001b[1;m minnesota indeed \u001b[1;42mthe\u001b[1;m talent \u001b[1;42mis\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43mthat\u001b[1;m s \u001b[1;43mabout\u001b[1;m \u001b[1;43mall\u001b[1;m \u001b[1;43mhe\u001b[1;m \u001b[1;43mhas\u001b[1;m \u001b[1;41mworking\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;43mhis\u001b[1;m favor \u001b[1;42mthe\u001b[1;m \u001b[1;41mteam\u001b[1;m dished \u001b[1;43mout\u001b[1;m oodles \u001b[1;42mof\u001b[1;m \u001b[1;41mmoney\u001b[1;m \u001b[1;42mfor\u001b[1;m latavius murray \u001b[1;43mwho\u001b[1;m \u001b[1;43mwill\u001b[1;m undoubtedly \u001b[1;43mbe\u001b[1;m \u001b[1;41minvolved\u001b[1;m \u001b[1;41mespecially\u001b[1;m \u001b[1;41mnear\u001b[1;m \u001b[1;42mthe\u001b[1;m stripe additionally \u001b[1;42mthe\u001b[1;m \u001b[1;41mline\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mamong\u001b[1;m \u001b[1;42mthe\u001b[1;m worst \u001b[1;42min\u001b[1;m football \u001b[1;42mand\u001b[1;m \u001b[1;43mthere\u001b[1;m s \u001b[1;41mlittle\u001b[1;m \u001b[1;41melse\u001b[1;m \u001b[1;41mreally\u001b[1;m threatening defenses \u001b[1;43mor\u001b[1;m creating td opportunities throw \u001b[1;42min\u001b[1;m \u001b[1;42ma\u001b[1;m bland vanilla scheme \u001b[1;42mand\u001b[1;m cook \u001b[1;43mwill\u001b[1;m \u001b[1;43mbe\u001b[1;m relying \u001b[1;42mon\u001b[1;m \u001b[1;43mhis\u001b[1;m \u001b[1;41mown\u001b[1;m highend abilities \u001b[1;42mto\u001b[1;m produce \u001b[1;43many\u001b[1;m fantasy \u001b[1;41mvalue\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;41m2017\u001b[1;m \u001b[1;41malthough\u001b[1;m murray \u001b[1;43mhad\u001b[1;m \u001b[1;42mthe\u001b[1;m highest percentage \u001b[1;42mof\u001b[1;m carries \u001b[1;41minside\u001b[1;m \u001b[1;42mthe\u001b[1;m 5yard \u001b[1;41mline\u001b[1;m \u001b[1;43mlast\u001b[1;m \u001b[1;41mseason\u001b[1;m 818 \u001b[1;41mpercent\u001b[1;m \u001b[1;42min\u001b[1;m oakland \u001b[1;43mwe\u001b[1;m \u001b[1;43mdid\u001b[1;m nt \u001b[1;41mtake\u001b[1;m \u001b[1;42mthe\u001b[1;m severity \u001b[1;42mof\u001b[1;m \u001b[1;43mhis\u001b[1;m ankle injury \u001b[1;43minto\u001b[1;m \u001b[1;41maccount\u001b[1;m \u001b[1;41mbefore\u001b[1;m grading \u001b[1;43mout\u001b[1;m cook s \u001b[1;41msituation\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mreason\u001b[1;m \u001b[1;41mwhy\u001b[1;m murray \u001b[1;41mtook\u001b[1;m \u001b[1;41mmultiple\u001b[1;m \u001b[1;41mfree\u001b[1;m agent visits \u001b[1;43mwas\u001b[1;m \u001b[1;42mto\u001b[1;m prove \u001b[1;42mto\u001b[1;m \u001b[1;41mteams\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;41mperson\u001b[1;m \u001b[1;43mthat\u001b[1;m \u001b[1;42mthe\u001b[1;m bone spurs \u001b[1;42min\u001b[1;m \u001b[1;43mhis\u001b[1;m ankle \u001b[1;43mwere\u001b[1;m \u001b[1;43mnot\u001b[1;m \u001b[1;41mserious\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42mthe\u001b[1;m vikings \u001b[1;41mtook\u001b[1;m \u001b[1;42mthe\u001b[1;m bait \u001b[1;42mthe\u001b[1;m \u001b[1;41mformer\u001b[1;m raider underwent surgery \u001b[1;42mon\u001b[1;m \u001b[1;43mhis\u001b[1;m ankle \u001b[1;42ma\u001b[1;m \u001b[1;41mweek\u001b[1;m \u001b[1;43mafter\u001b[1;m signing \u001b[1;42mand\u001b[1;m \u001b[1;43mwas\u001b[1;m \u001b[1;41mexpected\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;41mready\u001b[1;m \u001b[1;42mfor\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mstart\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mtraining\u001b[1;m camp fast \u001b[1;41mforward\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mstart\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;41mtraining\u001b[1;m camp \u001b[1;42mand\u001b[1;m latavius murray \u001b[1;42mis\u001b[1;m \u001b[1;41mstill\u001b[1;m mia \u001b[1;42min\u001b[1;m \u001b[1;41mfact\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;41mjuly\u001b[1;m 31st murray \u001b[1;41mput\u001b[1;m \u001b[1;43mno\u001b[1;m timetable \u001b[1;42mon\u001b[1;m \u001b[1;43mhis\u001b[1;m return \u001b[1;42mand\u001b[1;m \u001b[1;43mwould\u001b[1;m \u001b[1;43monly\u001b[1;m commit \u001b[1;42mto\u001b[1;m \u001b[1;42mthe\u001b[1;m regular \u001b[1;41mseason\u001b[1;m \u001b[1;43mafter\u001b[1;m missing \u001b[1;42mthe\u001b[1;m \u001b[1;41mentire\u001b[1;m offseason \u001b[1;41mprogram\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mstart\u001b[1;m \u001b[1;42mof\u001b[1;m camp \u001b[1;43mwith\u001b[1;m \u001b[1;43mhis\u001b[1;m \u001b[1;43mnew\u001b[1;m \u001b[1;41mteam\u001b[1;m \u001b[1;42ma\u001b[1;m frustrated murray stated \u001b[1;43mthat\u001b[1;m \u001b[1;43mhe\u001b[1;m s \u001b[1;41mbehind\u001b[1;m \u001b[1;42mthe\u001b[1;m 8ball \u001b[1;43mhe\u001b[1;m \u001b[1;41mwent\u001b[1;m onto \u001b[1;41msay\u001b[1;m \u001b[1;43mif\u001b[1;m \u001b[1;43mi\u001b[1;m m \u001b[1;43mnot\u001b[1;m healthy \u001b[1;43mthere\u001b[1;m s \u001b[1;43mno\u001b[1;m \u001b[1;41mpoint\u001b[1;m \u001b[1;42min\u001b[1;m \u001b[1;43mme\u001b[1;m \u001b[1;41mbeing\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mif\u001b[1;m \u001b[1;43mi\u001b[1;m m \u001b[1;43mnot\u001b[1;m \u001b[1;41mgood\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mi\u001b[1;m \u001b[1;41mca\u001b[1;m nt \u001b[1;41mhelp\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mteam\u001b[1;m regardless meanwhile \u001b[1;42mthe\u001b[1;m rookie \u001b[1;41mrunning\u001b[1;m \u001b[1;41mback\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;41mtaking\u001b[1;m \u001b[1;41mfull\u001b[1;m advantage \u001b[1;42mof\u001b[1;m \u001b[1;43mhis\u001b[1;m \u001b[1;41mopportunity\u001b[1;m \u001b[1;43mby\u001b[1;m \u001b[1;41mtaking\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mmajority\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mfirst\u001b[1;m \u001b[1;41mteam\u001b[1;m reps \u001b[1;42mand\u001b[1;m \u001b[1;43mhe\u001b[1;m s caught \u001b[1;42mthe\u001b[1;m \u001b[1;41mattention\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;43mhis\u001b[1;m teammates \u001b[1;42mand\u001b[1;m coaches \u001b[1;42mthe\u001b[1;m \u001b[1;43mfirst\u001b[1;m \u001b[1;41mthing\u001b[1;m \u001b[1;42mthe\u001b[1;m veteran \u001b[1;41mplayers\u001b[1;m \u001b[1;43mi\u001b[1;m ve talked \u001b[1;42mto\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;43mhim\u001b[1;m \u001b[1;41msay\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mthis\u001b[1;m guy \u001b[1;41mgets\u001b[1;m \u001b[1;42mit\u001b[1;m coach mike zimmer explained \u001b[1;43mafter\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mpractice\u001b[1;m \u001b[1;43mhe\u001b[1;m understands protections \u001b[1;43mhe\u001b[1;m \u001b[1;41mworks\u001b[1;m \u001b[1;41mhard\u001b[1;m \u001b[1;43mthey\u001b[1;m \u001b[1;41msee\u001b[1;m \u001b[1;43mhow\u001b[1;m \u001b[1;43mhe\u001b[1;m interacts \u001b[1;42min\u001b[1;m \u001b[1;42mthe\u001b[1;m locker \u001b[1;41mroom\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthat\u001b[1;m s \u001b[1;41mpart\u001b[1;m \u001b[1;42mof\u001b[1;m \u001b[1;42mit\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthen\u001b[1;m \u001b[1;43mwhen\u001b[1;m \u001b[1;43myou\u001b[1;m \u001b[1;43mhave\u001b[1;m \u001b[1;42ma\u001b[1;m \u001b[1;41mspecial\u001b[1;m playerlike \u001b[1;43mwhen\u001b[1;m \u001b[1;43mwe\u001b[1;m \u001b[1;41mgot\u001b[1;m linebacker anthony barrthey \u001b[1;41msay\u001b[1;m hey \u001b[1;41mman\u001b[1;m \u001b[1;43mthis\u001b[1;m guy \u001b[1;42mis\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;43mthan\u001b[1;m \u001b[1;43mother\u001b[1;m guys \u001b[1;43mthat\u001b[1;m s kinda \u001b[1;43mhow\u001b[1;m \u001b[1;43mhe\u001b[1;m \u001b[1;42mis\u001b[1;m \u001b[1;43mthey\u001b[1;m \u001b[1;41msee\u001b[1;m \u001b[1;43mhim\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;42mon\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;41mfield\u001b[1;m \u001b[1;43mwith\u001b[1;m \u001b[1;42mthe\u001b[1;m \u001b[1;43mother\u001b[1;m guys \u001b[1;42mand\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;43mlike\u001b[1;m \u001b[1;43mthere\u001b[1;m s \u001b[1;41msomething\u001b[1;m \u001b[1;41mdifferent\u001b[1;m \u001b[1;43mabout\u001b[1;m \u001b[1;43mthis\u001b[1;m guy \u001b[1;42mthe\u001b[1;m \u001b[1;41mway\u001b[1;m \u001b[1;43mhe\u001b[1;m runs accelerates \u001b[1;42mthe\u001b[1;m creases \u001b[1;43mhe\u001b[1;m \u001b[1;43mcan\u001b[1;m \u001b[1;43mget\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;43mhe\u001b[1;m s \u001b[1;41mgot\u001b[1;m \u001b[1;42ma\u001b[1;m tough mentality \u001b[1;41mplayers\u001b[1;m \u001b[1;43mcan\u001b[1;m \u001b[1;41msee\u001b[1;m exceptional athletes \u001b[1;43mwhen\u001b[1;m \u001b[1;43mthey\u001b[1;m \u001b[1;41mgo\u001b[1;m \u001b[1;43mout\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;42mand\u001b[1;m \u001b[1;43mthey\u001b[1;m re \u001b[1;41mgoing\u001b[1;m \u001b[1;41magainst\u001b[1;m guys \u001b[1;43mthey\u001b[1;m \u001b[1;43mcan\u001b[1;m \u001b[1;41msee\u001b[1;m \u001b[1;43mthis\u001b[1;m guy \u001b[1;42mis\u001b[1;m \u001b[1;41mpretty\u001b[1;m \u001b[1;41mgood\u001b[1;m \u001b[1;43mas\u001b[1;m \u001b[1;43mone\u001b[1;m \u001b[1;43mcan\u001b[1;m \u001b[1;41msee\u001b[1;m \u001b[1;42mit\u001b[1;m s \u001b[1;41msafe\u001b[1;m \u001b[1;42mto\u001b[1;m \u001b[1;41msay\u001b[1;m \u001b[1;43mthat\u001b[1;m dalvin cook \u001b[1;43mhas\u001b[1;m earned \u001b[1;42mthe\u001b[1;m \u001b[1;41mstaff\u001b[1;m s trust \u001b[1;42mand\u001b[1;m \u001b[1;43mwith\u001b[1;m murray \u001b[1;41mstill\u001b[1;m sidelined \u001b[1;43mhe\u001b[1;m s truly entering \u001b[1;42ma\u001b[1;m workhorse \u001b[1;41mrole\u001b[1;m \u001b[1;42mthe\u001b[1;m cause \u001b[1;42mfor\u001b[1;m concern \u001b[1;42mis\u001b[1;m \u001b[1;41mstill\u001b[1;m \u001b[1;43mthere\u001b[1;m \u001b[1;43mas\u001b[1;m \u001b[1;43mhe\u001b[1;m \u001b[1;43mhas\u001b[1;m \u001b[1;42ma\u001b[1;m tendency \u001b[1;42mto\u001b[1;m fumble \u001b[1;42mand\u001b[1;m \u001b[1;41monce\u001b[1;m murray joins \u001b[1;42mthe\u001b[1;m \u001b[1;41mteam\u001b[1;m \u001b[1;43mhe\u001b[1;m s \u001b[1;41mlikely\u001b[1;m \u001b[1;42mto\u001b[1;m vulture 710 touchdowns \u001b[1;43mfrom\u001b[1;m \u001b[1;42mthe\u001b[1;m 5foot10 210 lb rookie \u001b[1;42mthe\u001b[1;m offensive \u001b[1;41mline\u001b[1;m \u001b[1;41mstill\u001b[1;m sucks \u001b[1;42mand\u001b[1;m \u001b[1;42mthe\u001b[1;m scheme \u001b[1;42mis\u001b[1;m \u001b[1;41mstill\u001b[1;m \u001b[1;43mas\u001b[1;m vanilla \u001b[1;43mas\u001b[1;m \u001b[1;43mcan\u001b[1;m \u001b[1;43mbe\u001b[1;m \u001b[1;43mbut\u001b[1;m \u001b[1;43mwith\u001b[1;m \u001b[1;43mhis\u001b[1;m elite pass catching skills \u001b[1;43mhe\u001b[1;m \u001b[1;43mcould\u001b[1;m definitely \u001b[1;41msee\u001b[1;m 250plus touches \u001b[1;42min\u001b[1;m \u001b[1;41mfact\u001b[1;m vikings beat writer matt vensel \u001b[1;42mof\u001b[1;m \u001b[1;42mthe\u001b[1;m star tribune \u001b[1;43msaid\u001b[1;m \u001b[1;43mdo\u001b[1;m nt rule \u001b[1;43mout\u001b[1;m 300 touches \u001b[1;42mfor\u001b[1;m \u001b[1;42mthe\u001b[1;m rookie cook \u001b[1;43mwill\u001b[1;m \u001b[1;41mbring\u001b[1;m bigplay \u001b[1;41mability\u001b[1;m \u001b[1;41m10\u001b[1;m 50plus yard runs \u001b[1;43mat\u001b[1;m florida \u001b[1;41mstate\u001b[1;m elite \u001b[1;41mhands\u001b[1;m \u001b[1;42mfor\u001b[1;m \u001b[1;42ma\u001b[1;m runner 79 receptions \u001b[1;42min\u001b[1;m \u001b[1;41mthree\u001b[1;m seasons \u001b[1;42mand\u001b[1;m everydown \u001b[1;41mpotential\u001b[1;m 766 touches \u001b[1;42min\u001b[1;m \u001b[1;41mthree\u001b[1;m seasons \u001b[1;42mto\u001b[1;m \u001b[1;42ma\u001b[1;m vikings \u001b[1;41mteam\u001b[1;m \u001b[1;43mthat\u001b[1;m desperately \u001b[1;41mneeds\u001b[1;m \u001b[1;42ma\u001b[1;m legitimate playmaker \u001b[1;43mdo\u001b[1;m nt forget \u001b[1;43mabout\u001b[1;m \u001b[1;43mhis\u001b[1;m blazing \u001b[1;41mspeed\u001b[1;m \u001b[1;43mwhich\u001b[1;m \u001b[1;43mwas\u001b[1;m \u001b[1;42mon\u001b[1;m display \u001b[1;43mwhen\u001b[1;m \u001b[1;43mhe\u001b[1;m dusted cornerback xavier rhodes  \u001b[1;42mand\u001b[1;m \u001b[1;43mhis\u001b[1;m 44 \u001b[1;41m40\u001b[1;m \u001b[1;41mspeed\u001b[1;m  \u001b[1;41mdown\u001b[1;m \u001b[1;42mthe\u001b[1;m sidelines \u001b[1;41mduring\u001b[1;m \n"
     ]
    }
   ],
   "source": [
    "print(web_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reals = df_real_1[['Rank', 'Frequency']]\n",
    "df_reals['Tot rank'] = df_reals['Rank'].rank(method='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Tot rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <th>DT</th>\n",
       "      <td>1.0</td>\n",
       "      <td>57448.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <th>TO</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27899.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <th>IN</th>\n",
       "      <td>3.0</td>\n",
       "      <td>26095.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <th>CC</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25389.7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <th>DT</th>\n",
       "      <td>5.0</td>\n",
       "      <td>23446.6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectaclehis</th>\n",
       "      <th>JJ</th>\n",
       "      <td>211743.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>179737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townsquare</th>\n",
       "      <th>NNP</th>\n",
       "      <td>211927.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>179738.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rarecasualplay</th>\n",
       "      <th>NNP</th>\n",
       "      <td>212223.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>179739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unrewarding</th>\n",
       "      <th>VBG</th>\n",
       "      <td>213806.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>179740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kyriazis</th>\n",
       "      <th>NNP</th>\n",
       "      <td>214614.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>179741.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179741 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Rank  Frequency  Tot rank\n",
       "the            DT        1.0    57448.9       1.0\n",
       "to             TO        2.0    27899.9       2.0\n",
       "of             IN        3.0    26095.2       3.0\n",
       "and            CC        4.0    25389.7       4.0\n",
       "a              DT        5.0    23446.6       5.0\n",
       "...                      ...        ...       ...\n",
       "spectaclehis   JJ   211743.1        0.1  179737.0\n",
       "townsquare     NNP  211927.5        0.1  179738.0\n",
       "rarecasualplay NNP  212223.7        0.1  179739.0\n",
       "unrewarding    VBG  213806.5        0.1  179740.0\n",
       "kyriazis       NNP  214614.5        0.2  179741.0\n",
       "\n",
       "[179741 rows x 3 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
